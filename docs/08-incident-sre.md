# 장애 대응/SRE - IT 서비스 운영 필수 지식

> 이 문서는 IT 서비스 운영에 필요한 장애 대응 및 SRE(Site Reliability Engineering) 지식을 레벨별로 정리한 학습 자료입니다.

## 레벨 가이드
| 레벨 | 대상 | 설명 |
|------|------|------|
| ⭐ Level 1 | 입문 | 개념 이해, 기본 용어 |
| ⭐⭐ Level 2 | 주니어 | 실무 적용, 트러블슈팅 기초 |
| ⭐⭐⭐ Level 3 | 시니어 | 아키텍처 설계, 성능 최적화 |
| ⭐⭐⭐⭐ Level 4 | 리드/CTO | 전략적 의사결정, 대규모 설계 |

---

## 1. SRE 원칙

### 개념 설명
SRE(Site Reliability Engineering)는 Google에서 시작된 운영 방법론으로, 소프트웨어 엔지니어링 원칙을 인프라와 운영에 적용한다. "운영을 소프트웨어 문제로 다룬다"는 철학이 핵심이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - SRE의 정의: 소프트웨어 엔지니어가 운영 문제를 해결하는 방식
> - 신뢰성(Reliability): 시스템이 요구되는 기능을 정상적으로 수행하는 능력
> - Ops vs SRE: 수동 작업 vs 자동화와 엔지니어링
>
> **Q: SRE와 DevOps의 차이점은?**
> **A:** DevOps는 문화와 철학 중심의 광범위한 움직임이고, SRE는 그 철학을 구체적인 실천 방법과 메트릭으로 구현한 것이다. SRE는 "DevOps를 구현하는 하나의 방법"으로 볼 수 있다.
>
> **Q: SRE의 핵심 목표는?**
> **A:** 시스템의 신뢰성을 측정 가능하게 만들고, 자동화를 통해 수동 운영 작업(Toil)을 줄이며, 개발 속도와 안정성 사이의 균형을 맞추는 것이다.
>
> **Q: 왜 "소프트웨어 엔지니어링으로서의 운영"인가?**
> **A:** 문제를 일회성으로 해결하지 않고, 코드로 자동화하고, 재사용 가능한 솔루션을 만들어 반복적인 수동 작업을 제거한다.

> ⭐⭐ **Level 2 (주니어)**
> - 50% 규칙: SRE 시간의 50% 이상을 엔지니어링 작업에 투자
> - Toil 제거: 자동화로 반복 작업 감소
> - 점진적 롤아웃: 변경의 영향을 제한하는 배포 전략
>
> **Q: 50% 규칙이 중요한 이유는?**
> **A:** Toil이 50%를 넘으면 자동화할 시간이 없어져 악순환에 빠진다. 엔지니어링 시간을 확보해야 장기적으로 운영 부담이 줄어든다.
>
> **Q: SRE가 개발팀과 협력하는 방식은?**
> **A:** SLO를 함께 정의하고, 에러 버짓 내에서 개발 속도와 안정성 균형을 맞춘다. SRE는 프로덕션 인사이트를 개발팀에 제공하고 신뢰성 요구사항을 코드 리뷰한다.
>
> **Q: SRE 팀 구성 모델은?**
> **A:** 1) 중앙 집중형: 전사 SRE 팀 2) 임베디드: 각 제품팀에 SRE 배치 3) 하이브리드: 플랫폼 SRE + 임베디드 SRE. 조직 규모와 서비스 복잡도에 따라 선택.

> ⭐⭐⭐ **Level 3 (시니어)**
> - Production Readiness Review: 서비스 프로덕션 투입 전 점검
> - Capacity Planning: 수요 예측과 용량 계획
> - Emergency Response: 체계적인 장애 대응 프로세스
>
> **Q: Production Readiness Review에 포함되어야 할 항목은?**
> **A:** 아키텍처 검토, 장애 시나리오, 모니터링/알림, SLO 정의, 온콜 런북, 의존성 분석, 용량 계획, 보안 검토, 데이터 백업/복구 계획.
>
> **Q: SRE가 서비스를 "지원하지 않겠다"고 결정하는 기준은?**
> **A:** SLO 미정의, 반복되는 장애 미개선, 과도한 Toil 발생, 개발팀의 신뢰성 투자 거부. SRE는 유한한 리소스이므로 ROI 높은 서비스에 집중.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - SRE 조직 성숙도 모델: 조직 전체의 SRE 역량 평가
> - 신뢰성 문화: 엔지니어링 전반에 신뢰성 마인드셋 확산
>
> **Q: 조직에 SRE 문화를 도입하는 전략은?**
> **A:** 1) 파일럿 팀에서 성공 사례 만들기 2) SLO 기반 대화 문화 조성 3) 포스트모텀 공유 장려 4) Toil 측정 및 가시화 5) 경영진 지원 확보 6) 교육 프로그램 운영.
>
> **Q: SRE 투자의 ROI를 측정하는 방법은?**
> **A:** 장애로 인한 비용 감소, MTTR 개선, Toil 시간 감소, 개발자 생산성 향상, 고객 만족도/이탈률 변화를 정량화.

---

## 2. 온콜 운영

### 개념 설명
온콜(On-Call)은 서비스 장애 발생 시 즉각 대응할 수 있도록 엔지니어가 교대로 대기하는 체계이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 온콜: 업무 시간 외에도 장애 알림에 대응하는 역할
> - 로테이션: 온콜 담당자 교대 주기
> - 페이저: PagerDuty, Opsgenie 등 알림 시스템
>
> **Q: 온콜이 필요한 이유는?**
> **A:** 24/7 서비스는 언제든 장애가 발생할 수 있고, 빠른 대응이 비즈니스 영향과 사용자 경험에 직접적인 영향을 미친다.
>
> **Q: 온콜 담당자의 기본 책임은?**
> **A:** 알림 수신 및 응답, 초기 진단 및 완화, 에스컬레이션 판단, 장애 타임라인 기록, 핸드오프 수행.

> ⭐⭐ **Level 2 (주니어)**
> - Primary/Secondary: 1차/2차 대응자 체계
> - 에스컬레이션: 문제 해결이 안 될 때 상위 또는 전문가에게 넘기기
> - 핸드오프: 온콜 교대 시 진행 중인 이슈 인계
>
> **Q: Primary가 응답하지 않으면 어떻게 되는가?**
> **A:** 설정된 시간(보통 5-10분) 내 응답 없으면 Secondary에게 자동 에스컬레이션. 그래도 응답 없으면 매니저나 다음 레벨로 에스컬레이션.
>
> **Q: 효과적인 핸드오프 방법은?**
> **A:** 1) 현재 진행 중인 이슈 상태 문서화 2) 주의해야 할 알림이나 트렌드 공유 3) 예정된 변경사항 알림 4) 구두/비동기로 직접 소통.
>
> **Q: 온콜 중 알림 피로(Alert Fatigue)를 줄이는 방법은?**
> **A:** 불필요한 알림 제거, 알림 그룹화, 임계값 튜닝, 자동 복구 구현, 정보성 알림과 긴급 알림 분리.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 온콜 로드 밸런싱: 공정한 부담 분배
> - 온콜 보상: 수당, 대체 휴무
> - 번아웃 방지: 지속 가능한 온콜 체계 설계
>
> **Q: 건강한 온콜 로테이션 설계 원칙은?**
> **A:** 최소 8명 이상 풀(주 1회 이하 온콜), 연속 온콜 금지, 공휴일 균등 분배, 긴급 호출 횟수 모니터링, 온콜 부담 측정 및 조정.
>
> **Q: 온콜 인력 부족 시 대응 방법은?**
> **A:** 개발팀과 온콜 공유, 자동화로 알림 감소, 서비스 단순화, 외부 NOC 활용, 장기적으로 채용 확대.
>
> **Q: 온콜 효과성 측정 지표는?**
> **A:** 알림 횟수, 응답 시간, MTTR, False Positive 비율, 에스컬레이션 비율, 온콜 만족도 설문.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - Follow-the-Sun: 글로벌 팀의 시간대별 온콜 교대
> - 온콜 경제학: 온콜 비용 vs 장애 비용 분석
>
> **Q: Follow-the-Sun 모델의 장단점은?**
> **A:** 장점: 야간 온콜 없음, 삶의 질 향상. 단점: 팀 간 조율 복잡, 지식 분산, 핸드오프 오버헤드. 3개 이상 시간대에 팀 필요.
>
> **Q: 온콜 없이 운영할 수 있는 서비스 조건은?**
> **A:** 완전 자동 복구, 충분한 중복성, 낮은 비즈니스 임계성, 또는 관리형 서비스 사용. 현실적으로 완전한 무온콜은 어렵지만 온콜 부담 최소화는 가능.

---

## 3. 장애 등급 (Severity Level)

### 개념 설명
장애 등급은 장애의 영향 범위와 긴급성에 따라 분류하여, 적절한 대응 수준과 리소스를 할당하는 체계이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - SEV1 (Critical): 전체 서비스 중단, 즉각 대응 필요
> - SEV2 (High): 주요 기능 장애, 빠른 대응 필요
> - SEV3 (Medium): 일부 기능 영향, 업무 시간 내 대응
> - SEV4 (Low): 경미한 이슈, 일반 티켓 처리
>
> **Q: 장애 등급이 필요한 이유는?**
> **A:** 모든 장애를 같은 긴급도로 처리하면 리소스 낭비와 중요 장애 대응 지연이 발생한다. 우선순위에 따른 효율적 대응을 위해 필요.
>
> **Q: SEV1과 SEV2를 구분하는 기준은?**
> **A:** SEV1은 전체 또는 대다수 사용자 영향, 핵심 비즈니스 기능 완전 중단. SEV2는 상당수 사용자 영향이나 우회 가능, 또는 일부 핵심 기능만 영향.

> ⭐⭐ **Level 2 (주니어)**
> - 영향도 평가 기준: 사용자 수, 매출 영향, 기능 중요도
> - SLA별 대응 시간: 등급별 목표 응답/해결 시간
> - 다운그레이드/업그레이드: 상황 변화에 따른 등급 조정
>
> **Q: SEV 등급별 일반적인 대응 SLA는?**
> **A:** SEV1: 15분 내 응답, 4시간 내 완화. SEV2: 30분 내 응답, 8시간 내 완화. SEV3: 4시간 내 응답, 24시간 내 해결. SEV4: 24시간 내 응답.
>
> **Q: 장애 등급을 잘못 판단했을 때 어떻게 하는가?**
> **A:** 언제든 등급 조정 가능. 과소평가보다 과대평가가 낫다. 조정 시 이유를 명시하고 기록. 사후에 등급 기준 개선에 반영.
>
> **Q: 고객 영향 없는 내부 시스템 장애의 등급은?**
> **A:** 직접 영향은 없지만, 간접 영향(모니터링 불가, 배포 불가 등)과 잠재적 위험을 고려하여 평가. 일반적으로 SEV2-3 수준.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 비즈니스 임팩트 분석: 재무적 영향, 규정 준수, 평판
> - 다중 장애 시 우선순위: 동시 장애 발생 시 트리아지
>
> **Q: 비즈니스 임팩트를 정량화하는 방법은?**
> **A:** 분당/시간당 매출 손실, 영향받는 고객 수, SLA 위반 페널티, 규정 위반 벌금, 복구 비용, 평판 손상 추정.
>
> **Q: 동시에 여러 장애가 발생하면?**
> **A:** 가장 높은 SEV에 집중, 나머지는 병렬 처리하거나 순차 대응. 리소스가 부족하면 매니지먼트 에스컬레이션으로 우선순위 결정.

---

## 4. 장애 대응 프로세스

### 개념 설명
체계적인 장애 대응 프로세스는 감지(Detect) → 분류(Triage) → 완화(Mitigate) → 복구(Resolve) → 분석(Review) 단계로 구성된다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 감지(Detect): 모니터링/알림 시스템 또는 사용자 보고로 장애 인지
> - 분류(Triage): 장애 범위, 영향도, 등급 결정
> - 완화(Mitigate): 빠른 조치로 영향 최소화
> - 복구(Resolve): 근본 원인 해결로 정상화
> - 분석(Review): 원인 분석 및 재발 방지
>
> **Q: 완화와 복구의 차이는?**
> **A:** 완화는 응급 조치(롤백, 트래픽 우회 등)로 일단 서비스 정상화. 복구는 근본 원인 해결. 완화 먼저, 복구는 나중에 해도 됨.
>
> **Q: 장애 감지의 이상적인 방식은?**
> **A:** 고객이 알기 전에 모니터링 시스템이 먼저 감지하는 것. 고객 보고로 알게 되면 모니터링 개선이 필요하다는 신호.

> ⭐⭐ **Level 2 (주니어)**
> - 타임라인 기록: 모든 행동과 발견을 타임스탬프와 함께 기록
> - 워룸(War Room): 장애 대응을 위한 집중 커뮤니케이션 채널
> - 상태 페이지 업데이트: 고객에게 현황 공유
>
> **Q: 장애 타임라인에 기록해야 할 내용은?**
> **A:** 장애 인지 시점, 주요 발견사항, 시도한 조치와 결과, 에스컬레이션, 완화/복구 시점, 의사결정과 근거.
>
> **Q: 워룸의 기본 규칙은?**
> **A:** 역할 명확히(IC, 커뮤니케이터, 기록자), 관련 없는 대화 금지, 행동 전 의도 공유, 발견 즉시 공유, 정기 상태 요약.
>
> **Q: 상태 페이지에 어느 수준까지 공개하는가?**
> **A:** 영향받는 서비스, 현재 상태, 예상 복구 시간. 기술 세부사항이나 보안 관련 정보는 공개하지 않음. 진행 상황 정기 업데이트.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 장애 선언(Declare Incident): 공식적으로 장애 상황 선언
> - 복구 시간 예측: 이해관계자에게 ETA 제공
> - 장애 종료 기준: 언제 장애가 "해결됨"인가
>
> **Q: 장애를 공식 선언하는 기준은?**
> **A:** 고객 영향 확인, 빠른 해결이 어려워 보임, 여러 팀 협력 필요, SEV1/2 수준. 애매하면 선언하는 것이 낫다.
>
> **Q: 복구 시간 예측이 어려울 때는?**
> **A:** "조사 중"으로 시작, 원인 파악되면 업데이트. 불확실해도 최선의 추정 제공. 예측이 틀리면 즉시 업데이트.
>
> **Q: 장애 종료 조건은?**
> **A:** 고객 영향 제거, 메트릭 정상화, 근본 원인 파악(해결은 아니어도 됨), 추가 조치 불필요. 모니터링 강화 후 종료.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - 장애 대응 성숙도: 조직의 대응 역량 평가
> - 비상 대응 훈련: 정기적인 모의 훈련
>
> **Q: 장애 대응 성숙도를 높이는 방법은?**
> **A:** 1) 문서화된 프로세스 2) 정기 훈련 3) 포스트모텀 문화 4) 자동화된 도구 5) 명확한 역할 정의 6) 경험 공유 세션.

---

## 5. Incident Commander 역할

### 개념 설명
Incident Commander(IC)는 장애 대응을 총괄하는 역할로, 조율, 의사결정, 커뮤니케이션을 담당한다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - IC의 역할: 장애 대응의 중앙 조율자
> - IC는 직접 디버깅하지 않음: 조율과 의사결정에 집중
>
> **Q: IC가 필요한 이유는?**
> **A:** 여러 사람이 개별적으로 움직이면 혼란과 중복 작업이 발생한다. 한 명이 전체 상황을 파악하고 조율해야 효율적.
>
> **Q: IC의 핵심 책임은?**
> **A:** 상황 파악, 작업 할당, 의사결정, 에스컬레이션, 커뮤니케이션 조율, 타임라인 관리.

> ⭐⭐ **Level 2 (주니어)**
> - IC 외 역할: Communications Lead, Scribe, Subject Matter Expert
> - IC 인수인계: 장시간 장애 시 IC 교대
>
> **Q: Communications Lead의 역할은?**
> **A:** 외부 커뮤니케이션 담당 - 상태 페이지 업데이트, 고객 알림, 경영진 보고. IC가 기술적 조율에 집중할 수 있게 함.
>
> **Q: Scribe(기록자)의 역할은?**
> **A:** 실시간으로 타임라인 기록, 주요 발견과 조치 문서화. 나중에 포스트모텀의 기초 자료가 됨.
>
> **Q: IC 인수인계 시 포함해야 할 내용은?**
> **A:** 현재 상황 요약, 진행 중인 작업, 시도했던 것과 결과, 대기 중인 이슈, 주요 이해관계자, 다음 단계.

> ⭐⭐⭐ **Level 3 (시니어)**
> - IC 의사결정: 불완전한 정보로 빠른 결정
> - IC와 기술 리드 분리: 역할 충돌 방지
>
> **Q: IC가 기술적으로 깊이 관여하면 안 되는 이유는?**
> **A:** 특정 문제에 몰입하면 전체 상황 파악이 어려워진다. IC는 헬리콥터 뷰를 유지해야 함.
>
> **Q: IC가 결정하기 어려운 상황에서는?**
> **A:** 가용한 정보로 최선의 판단, 판단 근거 기록, 잘못되면 즉시 수정. "결정 안 함"이 최악의 선택.
>
> **Q: IC 훈련 방법은?**
> **A:** 시뮬레이션 훈련, 소규모 장애에서 경험, 선임 IC 섀도잉, 포스트모텀 검토, 장애 대응 패턴 학습.

---

## 6. 포스트모텀

### 개념 설명
포스트모텀(Postmortem)은 장애 후 원인 분석, 교훈 도출, 재발 방지책을 문서화하는 과정이다. Blameless(비난 없는) 문화가 핵심이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 포스트모텀 목적: 학습과 개선, 비난이 아님
> - Blameless: 개인을 비난하지 않고 시스템 개선에 집중
>
> **Q: 포스트모텀을 작성하는 이유는?**
> **A:** 같은 장애 반복 방지, 조직 학습, 장애 대응 프로세스 개선, 지식 공유.
>
> **Q: Blameless 포스트모텀이란?**
> **A:** "누가 잘못했나"가 아니라 "시스템이 왜 이런 실수를 허용했나"에 집중. 사람은 실수하므로 시스템이 실수를 방지/감지해야 함.
>
> **Q: 모든 장애에 포스트모텀이 필요한가?**
> **A:** SEV1/2는 필수, SEV3는 학습 가치 있으면 작성. 간단한 이슈는 간략한 "mini postmortem" 가능.

> ⭐⭐ **Level 2 (주니어)**
> - 포스트모텀 구성요소: 요약, 타임라인, 근본 원인, 영향, 액션 아이템
> - 5 Whys: 근본 원인 도출 기법
> - 액션 아이템: 구체적이고 할당된 개선 작업
>
> **Q: 5 Whys 기법은 어떻게 적용하는가?**
> **A:** "왜 이런 일이 발생했나?"를 반복하여 표면적 원인에서 근본 원인까지 도달. 5회가 아니어도 되고, 여러 분기로 나뉠 수 있음.
>
> **Q: 좋은 액션 아이템의 조건은?**
> **A:** 구체적(무엇을), 측정 가능, 담당자 지정, 기한 설정, 추적 가능. "모니터링 개선"(X) → "API 응답시간 알림 임계값 500ms로 조정 by 3/15"(O).
>
> **Q: 포스트모텀 타임라인 작성 시 주의점은?**
> **A:** 객관적 사실만 기록, 판단이나 비난 배제, 시간대(타임존) 명시, 중요 이벤트만 포함, 너무 상세하지 않게.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 기여 요인(Contributing Factors): 근본 원인 외 영향을 준 요소들
> - 포스트모텀 리뷰 미팅: 이해관계자와 함께 검토
> - 트렌드 분석: 반복되는 패턴 식별
>
> **Q: "근본 원인"이 하나가 아닐 때는?**
> **A:** 복잡한 장애는 여러 원인이 결합. "근본 원인"보다 "기여 요인들"로 표현하는 것이 더 정확. 각 요인별 개선책 도출.
>
> **Q: 포스트모텀 액션 아이템 완료율을 높이는 방법은?**
> **A:** 정기적 추적 미팅, 백로그에 통합, 우선순위 지정, 경영진 리뷰, 완료율 메트릭화, 미완료 시 원인 분석.
>
> **Q: 포스트모텀의 공유 범위는?**
> **A:** 기본적으로 전사 공유 권장. 보안 민감 정보는 편집. 외부 공개 포스트모텀은 고객 신뢰 구축에 도움(GitLab, Cloudflare 사례).

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - 포스트모텀 문화 정착: 조직 전체에 학습 문화 확산
> - 학습 조직: 장애를 개선 기회로 활용
>
> **Q: 포스트모텀 문화가 정착되지 않는 원인은?**
> **A:** 비난 문화, 시간 부족, 경영진 무관심, 액션 미이행, 형식적 진행. 리더십의 명시적 지지와 모범이 필요.
>
> **Q: 포스트모텀에서 학습을 극대화하는 방법은?**
> **A:** 전사 공유 세션, 유사 장애 패턴 분석, 런북 업데이트 자동화, 신규 입사자 온보딩에 활용, 분기별 장애 트렌드 리뷰.

---

## 7. SLI/SLO/SLA

### 개념 설명
SLI(Service Level Indicator)는 서비스 품질 측정 지표, SLO(Service Level Objective)는 목표치, SLA(Service Level Agreement)는 고객과의 계약이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - SLI: 서비스 수준 지표 (예: 응답 시간, 에러율, 가용성)
> - SLO: 서비스 수준 목표 (예: 99.9% 가용성, p99 응답시간 200ms 이하)
> - SLA: 서비스 수준 협약 (SLO 미달 시 보상/페널티 포함)
>
> **Q: SLI, SLO, SLA의 관계는?**
> **A:** SLI는 "무엇을 측정하는가", SLO는 "얼마나 좋아야 하는가", SLA는 "달성 못하면 어떻게 되는가". SLI를 정의하고, SLO를 설정하고, 필요시 SLA로 계약.
>
> **Q: 99.9%와 99.99% 가용성의 다운타임 차이는?**
> **A:** 99.9%(Three 9s): 연간 8.76시간 다운타임. 99.99%(Four 9s): 연간 52.56분. 9 하나 추가는 10배 어려움.
>
> **Q: 왜 100% SLO를 목표로 하면 안 되는가?**
> **A:** 100%는 불가능하고, 추구하면 모든 변경이 위험해져 혁신이 멈춘다. 적절한 신뢰성 수준이 개발 속도와 균형을 맞춘다.

> ⭐⭐ **Level 2 (주니어)**
> - 좋은 SLI의 특성: 사용자 경험 반영, 측정 가능, 의미 있는 임계값
> - SLI 유형: 가용성, 지연시간, 처리량, 정확성
>
> **Q: 가용성 SLI를 정의하는 방법은?**
> **A:** 성공 요청 수 / 전체 요청 수. "성공"의 정의가 중요: HTTP 5xx 제외, 타임아웃 제외 등. 사용자 관점에서 정의.
>
> **Q: 지연시간 SLI에서 평균 vs 백분위수?**
> **A:** 평균은 이상치에 왜곡됨. p50(중앙값), p90, p99 사용 권장. p99가 사용자 경험의 "최악의 케이스"를 반영.
>
> **Q: SLO 설정 시 고려사항은?**
> **A:** 사용자 기대치, 기술적 실현 가능성, 비용, 의존 서비스의 SLO. 너무 높으면 달성 불가, 너무 낮으면 의미 없음.

> ⭐⭐⭐ **Level 3 (시니어)**
> - SLO 기반 알림: SLO 소진율 기반 알림 (Burn Rate)
> - Multi-window, Multi-burn-rate Alert: 정교한 알림 전략
> - SLO 문서: SLO 정의, 측정 방법, 예외 사항 문서화
>
> **Q: 에러 버짓 소진율(Burn Rate) 기반 알림이란?**
> **A:** "현재 속도로 에러가 발생하면 SLO를 언제 위반하는가"를 계산. 1시간에 30일 버짓의 2%를 소진하면 긴급, 6시간에 5%면 경고 등.
>
> **Q: SLO를 달성하고 있는데도 사용자 불만이 있다면?**
> **A:** SLI가 사용자 경험을 제대로 반영하지 않음. SLI 재정의 필요. 예: 평균 응답시간은 좋지만 특정 기능의 p99가 나쁜 경우.
>
> **Q: 외부 의존성이 SLO에 미치는 영향은?**
> **A:** 서비스 SLO는 의존성 SLO보다 높을 수 없음. 의존성 SLO를 고려한 현실적인 SLO 설정. 의존성 장애는 "예외"로 처리할지 결정.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - SLO와 비즈니스 연계: SLO가 비즈니스 목표를 반영
> - SLA 협상: 고객사와 SLA 협상 시 고려사항
>
> **Q: SLO를 비즈니스 목표와 연계하는 방법은?**
> **A:** 가용성 99.9% → 99.99%로 올리면 매출 X% 증가, 고객 이탈 Y% 감소 등 정량화. 신뢰성 투자의 ROI 측정.
>
> **Q: SLA 협상 시 주의점은?**
> **A:** SLO보다 낮은 SLA 설정(버퍼), 측정 방법 명확히, 예외 조항, 보상 상한선, 보고 주기, 분쟁 해결 절차.

---

## 8. 에러 버짓

### 개념 설명
에러 버짓(Error Budget)은 SLO가 허용하는 장애/에러의 양을 수치화한 것이다. 100% - SLO = 에러 버짓.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 에러 버짓: 허용 가능한 에러/장애의 양
> - 계산: 99.9% SLO = 0.1% 에러 버짓 = 월 43.2분 다운타임 허용
>
> **Q: 에러 버짓이 필요한 이유는?**
> **A:** 개발 속도와 안정성 사이의 객관적 균형점. "신뢰성이 충분하면 빠르게 개발, 부족하면 안정화"의 기준.
>
> **Q: 에러 버짓이 남아있다는 것의 의미는?**
> **A:** 더 많은 실험, 더 빠른 배포, 더 공격적인 변경이 가능하다. 에러 버짓은 혁신의 여유분.

> ⭐⭐ **Level 2 (주니어)**
> - 버짓 소진 추적: 대시보드로 실시간 모니터링
> - 버짓 소진 속도: 현재 추세로 버짓이 언제 소진되는가
>
> **Q: 에러 버짓 계산 예시는?**
> **A:** 월간 100만 요청, SLO 99.9% → 에러 버짓 1000 실패 요청. 현재까지 300 실패 → 30% 소진, 70% 잔여.
>
> **Q: 에러 버짓을 빠르게 소진하고 있다면?**
> **A:** 신규 배포 중단, 위험 작업 연기, 안정화 작업 우선순위 상향, 원인 조사.
>
> **Q: 계획된 유지보수도 에러 버짓에 포함되는가?**
> **A:** 일반적으로 포함. 유지보수도 고객에게는 다운타임. 예외로 처리하려면 SLO 문서에 명시.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 에러 버짓 정책: 버짓 소진 시 취할 조치 정의
> - 개발팀-SRE 협업: 에러 버짓 기반 의사결정
>
> **Q: 에러 버짓 정책의 예시는?**
> **A:** 버짓 50% 소진: 신규 기능 배포 검토 강화. 75% 소진: 신규 배포 중단, 안정화 집중. 100% 소진: 기능 동결, 버짓 복구까지.
>
> **Q: 개발팀이 에러 버짓 정책에 반발하면?**
> **A:** 에러 버짓은 공동 목표임을 강조, 정책 수립에 개발팀 참여, 데이터 기반 논의, 비즈니스 영향 설명.
>
> **Q: 에러 버짓이 남아도는 경우는?**
> **A:** SLO가 너무 낮거나 서비스가 매우 안정적. 더 공격적인 실험 가능하거나 SLO 상향 검토.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - 에러 버짓의 조직 문화적 의미
> - 에러 버짓과 비즈니스 의사결정
>
> **Q: 에러 버짓 문화의 조직적 이점은?**
> **A:** 개발팀과 운영팀 갈등 해소, 객관적 의사결정 기준, 신뢰성 투자 정당화, 리스크의 정량화.
>
> **Q: 비즈니스 이벤트(블랙프라이데이 등) 전 에러 버짓 관리는?**
> **A:** 이벤트 전 버짓 충분히 확보, 변경 동결 기간, 용량 추가 확보, 대응 인력 증원. 이벤트 중 발생한 에러도 버짓에 포함.

---

## 9. Chaos Engineering

### 개념 설명
Chaos Engineering은 시스템의 약점을 사전에 발견하기 위해 의도적으로 장애를 주입하는 방법론이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - Chaos Engineering: 의도적 장애 주입으로 시스템 회복력 테스트
> - Netflix Chaos Monkey: 무작위로 인스턴스 종료하는 도구
>
> **Q: Chaos Engineering의 목적은?**
> **A:** 실제 장애 전에 약점 발견, 시스템 회복력 검증, 운영팀 대응 연습, 가정 검증.
>
> **Q: 프로덕션에서 장애를 일으켜도 되는가?**
> **A:** 통제된 방식으로 한다. 영향 범위 제한, 즉시 중단 가능, 업무 시간 내 실행, 점진적 확대. 준비 없이 하면 안 됨.

> ⭐⭐ **Level 2 (주니어)**
> - Chaos 도구: Chaos Monkey, Gremlin, Litmus
> - Steady State: 시스템의 정상 상태 정의
> - Blast Radius: 실험 영향 범위 제한
>
> **Q: Chaos 실험 전 필요한 준비는?**
> **A:** Steady State 정의(정상 메트릭), 롤백 계획, 영향 범위 제한, 모니터링 준비, 이해관계자 알림.
>
> **Q: Steady State의 예시는?**
> **A:** 에러율 0.1% 미만, p99 응답시간 200ms 미만, 처리량 1000 RPS 이상. 실험 중 이 상태가 유지되는지 관찰.
>
> **Q: Blast Radius를 제한하는 방법은?**
> **A:** 일부 인스턴스/사용자만 대상, 짧은 실험 시간, 자동 중단 조건, 특정 리전/환경 한정.

> ⭐⭐⭐ **Level 3 (시니어)**
> - GameDay: 팀 전체가 참여하는 장애 대응 훈련
> - 장애 주입 유형: 인스턴스 종료, 네트워크 지연, CPU/메모리 압박, 의존성 장애
> - 실험 자동화: CI/CD에 Chaos 테스트 통합
>
> **Q: GameDay 진행 방법은?**
> **A:** 1) 시나리오 설계 2) 역할 배정(공격팀/방어팀) 3) 모니터링 준비 4) 장애 주입 5) 대응 관찰 6) 디브리핑 7) 개선점 도출.
>
> **Q: 어떤 장애 시나리오를 실험해야 하는가?**
> **A:** 과거 장애 재현, 의존성 장애, 급격한 트래픽 증가, 데이터센터/AZ 장애, 데이터 손상, 느린 의존성.
>
> **Q: Chaos Engineering 성숙도 단계는?**
> **A:** 1) Ad-hoc 실험 2) 정기적 실험 3) 프로덕션 실험 4) CI/CD 통합 5) 지속적 Chaos.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - Chaos Engineering 문화: 조직 전체 도입
> - 규제 환경에서의 Chaos: 금융, 의료 등
>
> **Q: 조직에 Chaos Engineering 도입 전략은?**
> **A:** 1) 작은 규모로 시작 2) 성공 사례 공유 3) 점진적 확대 4) 도구 표준화 5) 교육 6) 경영진 지지 확보.
>
> **Q: 규제 환경에서 Chaos Engineering을 하려면?**
> **A:** 규제 기관 사전 협의, 문서화된 절차, 비프로덕션 환경 우선, 영향 범위 철저히 제한, 감사 로그.

---

## 10. 카나리 분석

### 개념 설명
카나리 분석(Canary Analysis)은 새 버전을 소규모 트래픽에 노출시키고 메트릭을 기존 버전과 비교하여 안전성을 자동 판단하는 기법이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 카나리 배포: 새 버전을 일부 트래픽에만 먼저 배포
> - 카나리 분석: 카나리와 베이스라인 메트릭 비교
>
> **Q: 카나리 배포의 장점은?**
> **A:** 전체 사용자에게 영향 주기 전에 문제 발견, 빠른 롤백 가능, 점진적 신뢰 구축.
>
> **Q: 어떤 메트릭을 비교하는가?**
> **A:** 에러율, 지연시간(p50, p99), CPU/메모리 사용량, 비즈니스 메트릭(전환율 등).

> ⭐⭐ **Level 2 (주니어)**
> - 베이스라인 vs 카나리: 같은 조건에서 이전 버전과 새 버전 비교
> - 수동 vs 자동 분석: 대시보드 확인 vs 자동화된 판단
>
> **Q: 베이스라인은 왜 필요한가?**
> **A:** 프로덕션 환경은 시간에 따라 변동. 프로덕션 전체 대비가 아니라 같은 시점의 베이스라인과 비교해야 정확.
>
> **Q: 카나리 트래픽 비율은 얼마가 적당한가?**
> **A:** 보통 1-5%로 시작. 통계적 유의성 확보에 충분한 트래픽, 하지만 문제 시 영향 최소화. 서비스 규모에 따라 조정.
>
> **Q: 카나리 분석 기간은 얼마가 적당한가?**
> **A:** 최소 15-30분, 이상적으로 1-2시간. 트래픽 패턴 주기(시간대별, 요일별) 고려. 메모리 누수 같은 지연 문제 고려.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 자동화된 카나리 분석: Kayenta, Argo Rollouts 분석
> - 통계적 비교: Mann-Whitney U 테스트 등
> - 롤백 기준: 분석 결과에 따른 자동/수동 롤백
>
> **Q: Kayenta(카옌타)의 동작 원리는?**
> **A:** 베이스라인과 카나리의 메트릭을 수집, 통계적으로 비교, 점수 산출, 임계값에 따라 Pass/Fail 판정.
>
> **Q: 통계적 비교가 왜 필요한가?**
> **A:** 단순 평균 비교는 노이즈에 취약. 통계적 검정으로 차이가 유의미한지 판단. False Positive/Negative 감소.
>
> **Q: 카나리 분석이 Pass했지만 문제가 발생하면?**
> **A:** 메트릭 커버리지 부족. SLI에 포함되지 않은 문제. 분석 기간 부족. 포스트모텀에서 분석하고 메트릭/분석 개선.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - 멀티 리전 카나리: 리전별 순차 카나리
> - 비즈니스 메트릭 통합: 기술 메트릭 + 비즈니스 KPI
>
> **Q: 기술 메트릭만으로 충분하지 않은 경우는?**
> **A:** 기술적으로 정상이지만 UX 저하, 전환율 감소 등 비즈니스 영향. 비즈니스 메트릭을 카나리 분석에 포함.

---

## 11. 런북 (Runbook)

### 개념 설명
런북은 특정 상황(장애, 알림)에 대한 대응 절차를 문서화한 것이다. 신속하고 일관된 대응을 가능하게 한다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 런북: 장애 대응 절차서
> - 알림별 런북: 각 알림에 대한 대응 절차 연결
>
> **Q: 런북이 필요한 이유는?**
> **A:** 장애 시 당황하지 않고 체계적 대응, 경험 적은 담당자도 대응 가능, 일관된 대응 품질.
>
> **Q: 좋은 런북의 특징은?**
> **A:** 명확한 단계, 복붙 가능한 명령어, 의사결정 기준 포함, 에스컬레이션 경로, 정기 업데이트.

> ⭐⭐ **Level 2 (주니어)**
> - 런북 구성요소: 개요, 영향, 진단 단계, 해결 단계, 에스컬레이션
> - 의사결정 트리: 상황별 분기 처리
>
> **Q: 런북에 포함되어야 할 필수 항목은?**
> **A:** 알림 의미, 영향 범위, 진단 명령어/방법, 해결 절차, 확인 방법, 에스컬레이션 기준 및 연락처, 관련 문서 링크.
>
> **Q: 의사결정 트리 작성 방법은?**
> **A:** "이 조건이면 A, 아니면 B" 형태. 플로우차트로 시각화. 복잡한 분기는 단순화하거나 에스컬레이션.
>
> **Q: 런북은 어디에 저장하는가?**
> **A:** 알림에서 바로 링크, 빠르게 검색 가능한 위치. 위키, Confluence, Git 등. 장애 시 접근 가능해야 함(인프라 장애 시에도).

> ⭐⭐⭐ **Level 3 (시니어)**
> - 자동화 런북: 스크립트로 자동 실행
> - 런북 테스트: 정기적으로 런북 유효성 검증
> - 런북 업데이트: 포스트모텀 후 개선
>
> **Q: 런북을 자동화하는 방법은?**
> **A:** 진단 스크립트, 원클릭 복구 도구, PagerDuty/Rundeck 등 런북 자동화 플랫폼, ChatOps 통합.
>
> **Q: 런북이 outdated 되는 것을 방지하려면?**
> **A:** 장애 대응 시 런북 유효성 체크, 포스트모텀 액션에 런북 업데이트 포함, 정기 리뷰 일정, 담당자 지정.
>
> **Q: 런북 테스트 방법은?**
> **A:** GameDay에서 런북 따라 대응, 신규 입사자가 런북으로 대응 시뮬레이션, 분기별 런북 워크스루.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - AI 기반 런북: 자동 제안, 지능형 라우팅
>
> **Q: 런북의 미래 방향은?**
> **A:** ML 기반 장애 패턴 인식, 자동 진단 및 복구 제안, 과거 장애와 유사성 분석, 대화형 런북 가이드.

---

## 12. 용량 계획 (Capacity Planning)

### 개념 설명
용량 계획은 미래 수요를 예측하고 이를 충족할 충분한 리소스를 확보하는 프로세스이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 용량: CPU, 메모리, 스토리지, 네트워크 등 리소스
> - 용량 계획: 미래 수요를 예측하고 준비
>
> **Q: 용량 계획이 중요한 이유는?**
> **A:** 부족하면 성능 저하/장애, 과다하면 비용 낭비. 적절한 타이밍에 적절한 양의 용량 확보 필요.
>
> **Q: 용량 부족의 징후는?**
> **A:** 응답 시간 증가, CPU/메모리 사용률 상승 추세, 큐 길이 증가, 에러율 증가.

> ⭐⭐ **Level 2 (주니어)**
> - 수요 예측: 과거 데이터, 성장률, 이벤트 기반 예측
> - 로드 테스트: k6, Locust, JMeter 등으로 용량 측정
>
> **Q: 수요 예측에 사용하는 데이터는?**
> **A:** 과거 트래픽 패턴, 사업 성장률, 계절성, 마케팅 이벤트, 신규 기능 출시 계획.
>
> **Q: 로드 테스트로 무엇을 알 수 있는가?**
> **A:** 현재 용량의 한계, 병목 지점, 스케일링 특성, 성능 저하 시작점.
>
> **Q: k6 vs JMeter 선택 기준은?**
> **A:** k6는 JavaScript 기반, 개발자 친화적, CI/CD 통합 용이. JMeter는 GUI 기반, 복잡한 시나리오, 다양한 프로토콜 지원.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 버스트 트래픽: 예상치 못한 급격한 트래픽 증가 대응
> - 비용 효율적 스케일링: 오토스케일링, Spot 인스턴스
> - N+2 용량: 장애 상황을 고려한 여유 용량
>
> **Q: 버스트 트래픽 대응 전략은?**
> **A:** 오토스케일링 설정, CDN 캐싱, 큐를 통한 부하 분산, 서킷브레이커, 우아한 성능 저하(Graceful Degradation).
>
> **Q: N+2 용량 계획이란?**
> **A:** 정상 부하의 N개 인스턴스 + 1개 장애 + 1개 배포/유지보수 = 최소 N+2. 고가용성을 위한 여유분.
>
> **Q: 용량 계획의 시간 범위는?**
> **A:** 단기(주간): 오토스케일링. 중기(월간): 용량 조정. 장기(분기/연간): 인프라 투자, 아키텍처 변경.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - 비용 최적화: FinOps 관점의 용량 계획
> - 멀티 클라우드 용량: 클라우드 간 용량 분산
>
> **Q: 용량과 비용의 균형을 맞추는 방법은?**
> **A:** Reserved/Savings Plans로 베이스라인 확보, Spot으로 버스트 처리, 사용량 모니터링 및 정기 right-sizing.
>
> **Q: 용량 계획에서 비즈니스와 협력하는 방법은?**
> **A:** 비즈니스 이벤트 캘린더 공유, 성장 전망 공유, 용량 한계 투명하게 소통, 용량 제약이 비즈니스에 미치는 영향 설명.

---

## 13. Toil

### 개념 설명
Toil은 서비스 운영에 필요하지만 장기적 가치가 없고, 수동적이며, 자동화 가능한 반복 작업이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - Toil 정의: 수동, 반복, 자동화 가능, 전술적, 장기 가치 없음
> - Toil vs Engineering: Toil 줄이기 = 엔지니어링 시간 확보
>
> **Q: Toil의 예시는?**
> **A:** 수동 배포, 비밀번호 재설정, 로그 정리, 인증서 갱신, 수동 스케일링, 반복적인 알림 대응.
>
> **Q: 모든 운영 작업이 Toil인가?**
> **A:** 아니다. 프로젝트 기획, 아키텍처 설계, 코드 리뷰, 포스트모텀 등은 엔지니어링 작업.

> ⭐⭐ **Level 2 (주니어)**
> - Toil 측정: 시간 추적, 카테고리화
> - Toil 우선순위: 빈도 x 소요시간 x 자동화 가능성
>
> **Q: Toil을 측정하는 방법은?**
> **A:** 시간 추적(시트, 도구), 알림 대응 시간 집계, 반복 작업 티켓 분석. 일주일 단위로 샘플링.
>
> **Q: Toil 자동화 우선순위를 정하는 기준은?**
> **A:** 빈도 높은 것, 시간 많이 드는 것, 자동화 쉬운 것 우선. ROI = (빈도 x 수동 소요시간) / 자동화 비용.
>
> **Q: Toil 제거 외에 Toil 관리 방법은?**
> **A:** Toil 분산(로테이션), 문서화(런북), 도구 개선, 셀프서비스 제공.

> ⭐⭐⭐ **Level 3 (시니어)**
> - Toil 버짓: 팀 전체 시간 중 Toil 비중 상한
> - Toil과 SRE 계약: 개발팀과 Toil 분담 협의
>
> **Q: Toil이 50%를 넘으면 어떻게 되는가?**
> **A:** 자동화할 시간이 없어 Toil이 더 증가하는 악순환. SRE 번아웃, 서비스 지원 거부 고려.
>
> **Q: 개발팀이 Toil을 유발하는 경우 대응은?**
> **A:** 데이터로 Toil 가시화, 개발팀과 함께 원인 분석, 자동화 우선순위 협의, Production Readiness Review에 반영.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - Toil과 조직 문화: Toil 인식 확산
>
> **Q: 조직 전체에 Toil 의식을 높이는 방법은?**
> **A:** Toil 대시보드 공유, 자동화 성과 발표, Toil 감소를 성과 지표에 포함, 개발팀도 운영 경험하게 하기.

---

## 14. 릴리스 엔지니어링

### 개념 설명
릴리스 엔지니어링은 소프트웨어를 신뢰성 있게, 예측 가능하게, 반복 가능하게 배포하는 프로세스와 도구를 다룬다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 릴리스: 새 버전의 소프트웨어를 프로덕션에 배포
> - 릴리스 프로세스: 빌드 → 테스트 → 배포 → 검증
>
> **Q: 릴리스 엔지니어링의 목표는?**
> **A:** 릴리스를 안전하고, 빠르고, 쉽게 만드는 것. 자주 배포할수록 각 릴리스의 위험이 줄어듦.
>
> **Q: 릴리스 주기는 어떻게 결정하는가?**
> **A:** 비즈니스 요구, 팀 역량, 자동화 수준에 따라. 빈번한 릴리스(일 여러 회) vs 정기 릴리스(주/월간).

> ⭐⭐ **Level 2 (주니어)**
> - 체리픽(Cherry-pick): 특정 커밋만 선별 배포
> - 핫픽스: 긴급 버그 수정 배포
> - 릴리스 브랜치: 릴리스용 별도 브랜치
>
> **Q: 체리픽이 필요한 상황은?**
> **A:** 메인 브랜치에 배포 불가능한 변경이 섞여 있을 때 특정 수정만 배포. 하지만 일반적으로 피해야 하는 패턴.
>
> **Q: 핫픽스 프로세스는?**
> **A:** 1) 문제 재현/확인 2) 수정 개발 3) 최소한의 테스트 4) 승인 5) 배포 6) 검증 7) 메인 브랜치에 머지. 위험하므로 최소화.
>
> **Q: 릴리스 브랜치 전략의 장단점은?**
> **A:** 장점: 안정성, 핫픽스 적용 용이. 단점: 브랜치 관리 복잡, 메인과 차이 발생 위험.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 릴리스 트레인: 정해진 주기로 준비된 기능 배포
> - 피처 플래그: 코드는 배포하되 기능은 비활성화
> - 릴리스 체크리스트: 배포 전 확인 항목
>
> **Q: 릴리스 트레인의 장점은?**
> **A:** 예측 가능한 일정, 놓치면 다음 기차, 기능별 독립적 개발 가능. 단, 피처 플래그와 병행해야 효과적.
>
> **Q: 피처 플래그의 위험은?**
> **A:** 플래그 관리 복잡성, 오래된 플래그 축적, 테스트 조합 증가, 성능 영향. 정기적인 플래그 정리 필요.
>
> **Q: 릴리스 체크리스트에 포함할 항목은?**
> **A:** 테스트 통과, 코드 리뷰 완료, 문서 업데이트, 롤백 계획, 모니터링 대시보드 준비, 온콜 통보.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - 릴리스 거버넌스: 조직 차원의 릴리스 정책
>
> **Q: 대규모 조직의 릴리스 조율 방법은?**
> **A:** 릴리스 캘린더, 변경 동결 기간, 의존성 조율 미팅, 릴리스 엔지니어 역할, 릴리스 자동화 플랫폼.

---

## 15. 재해 복구 (Disaster Recovery)

### 개념 설명
재해 복구(DR)는 대규모 장애(데이터센터 장애, 자연재해 등) 시 서비스를 복구하는 계획과 절차이다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - RTO (Recovery Time Objective): 복구 목표 시간
> - RPO (Recovery Point Objective): 데이터 손실 허용 범위
>
> **Q: RTO와 RPO의 차이는?**
> **A:** RTO는 "얼마나 빨리 복구해야 하는가"(시간), RPO는 "얼마나 많은 데이터 손실을 감수할 수 있는가"(시점).
>
> **Q: RTO 1시간, RPO 15분의 의미는?**
> **A:** 장애 후 1시간 내에 서비스 복구, 최대 15분치 데이터 손실 허용. 15분마다 백업 필요.

> ⭐⭐ **Level 2 (주니어)**
> - DR 전략: Backup/Restore, Pilot Light, Warm Standby, Multi-Site
> - 백업 전략: 전체/증분/차등 백업, 보관 기간
>
> **Q: DR 전략별 비용과 RTO 관계는?**
> **A:** Backup < Pilot Light < Warm Standby < Multi-Site. 비용이 높을수록 RTO가 짧음.
>
> **Q: Pilot Light 전략이란?**
> **A:** 핵심 시스템만 최소 규모로 DR 환경에 실행. 장애 시 스케일업하여 사용. 비용과 RTO의 균형점.
>
> **Q: 백업만으로 DR이 충분한가?**
> **A:** 소규모/저위험 서비스는 가능. 하지만 복구 시간이 길고, 백업 자체의 무결성 검증 필요. 주기적 복구 테스트 필수.

> ⭐⭐⭐ **Level 3 (시니어)**
> - 페일오버 테스트: DR 환경으로 전환 연습
> - 데이터 복제: 동기/비동기 복제, 복제 지연
> - DR 런북: 재해 시 절차 문서화
>
> **Q: 페일오버 테스트 주기는?**
> **A:** 최소 연 1회, 이상적으로 분기 1회. 테스트 없는 DR 계획은 작동하지 않을 가능성 높음.
>
> **Q: 동기 vs 비동기 데이터 복제의 트레이드오프는?**
> **A:** 동기: RPO=0 (데이터 손실 없음), 하지만 지연시간 증가. 비동기: 성능 좋음, 하지만 복제 지연만큼 데이터 손실 가능.
>
> **Q: DR 테스트 시 프로덕션 영향을 최소화하려면?**
> **A:** 트래픽 이중화, 읽기 전용 테스트, 점진적 트래픽 이동, 업무 시간 외 수행, 자동 롤백 준비.

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - DR 비용 정당화: 비즈니스 연속성 가치
> - 규정 준수: 산업별 DR 요구사항
>
> **Q: DR 투자 비용을 정당화하는 방법은?**
> **A:** 장애 시 비용(시간당 손실 x 예상 다운타임), 규정 위반 페널티, 평판 손상, 고객 이탈 vs DR 구축/운영 비용.
>
> **Q: 금융/의료 등 규제 산업의 DR 요구사항은?**
> **A:** 명시적 RTO/RPO, 문서화된 DR 계획, 정기 테스트 및 감사, 지리적 분산, 데이터 보안 유지.

---

## 16. 멀티 리전/AZ

### 개념 설명
멀티 리전/AZ(Availability Zone) 아키텍처는 지리적으로 분산된 인프라로 고가용성과 재해 복구를 실현한다.

### 레벨별 지식

> ⭐ **Level 1 (입문)**
> - 가용영역(AZ): 같은 리전 내 독립적인 데이터센터
> - 리전: 지리적으로 분리된 클라우드 위치
>
> **Q: AZ 분산이 필요한 이유는?**
> **A:** 단일 데이터센터 장애(정전, 네트워크, 화재)에 대한 보호. 같은 리전 내에서 저지연 유지하면서 가용성 향상.
>
> **Q: 리전과 AZ의 차이는?**
> **A:** AZ는 같은 리전 내 분리된 시설(저지연 연결), 리전은 지리적으로 멀리 떨어진 위치(높은 지연, 독립적).

> ⭐⭐ **Level 2 (주니어)**
> - Multi-AZ 배포: 여러 AZ에 인스턴스 분산
> - 로드밸런서: AZ 간 트래픽 분배
> - 데이터베이스 복제: Multi-AZ RDS 등
>
> **Q: Multi-AZ 웹 애플리케이션 구성 예시는?**
> **A:** ALB(여러 AZ), EC2 Auto Scaling Group(여러 AZ), RDS Multi-AZ, ElastiCache Multi-AZ.
>
> **Q: 하나의 AZ가 장애 나면?**
> **A:** 로드밸런서가 정상 AZ로만 트래픽 전송, Auto Scaling이 다른 AZ에 인스턴스 추가, RDS가 대기 인스턴스로 페일오버.
>
> **Q: Multi-AZ의 추가 비용은?**
> **A:** 인스턴스 비용 증가, AZ 간 데이터 전송 비용, 복제 스토리지 비용. 하지만 가용성 향상으로 장애 비용 절감.

> ⭐⭐⭐ **Level 3 (시니어)**
> - Active-Active 멀티 리전: 여러 리전에서 동시 서비스
> - Active-Passive 멀티 리전: 주 리전 + DR 리전
> - 글로벌 로드밸런싱: Route 53, CloudFront, Global Accelerator
>
> **Q: Active-Active vs Active-Passive 선택 기준은?**
> **A:** Active-Active: 낮은 지연 필요, 높은 가용성. Active-Passive: 비용 절감, 단순한 데이터 관리. Active-Active가 복잡하고 비용 높음.
>
> **Q: 멀티 리전에서 데이터 일관성 문제는?**
> **A:** 지연시간으로 인한 동기화 지연, 충돌 해결 필요. 최종 일관성 수용, 충돌 해결 로직, 또는 단일 리전 쓰기.
>
> **Q: 글로벌 로드밸런싱 전략은?**
> **A:** Latency-based routing(가장 빠른 리전), Geolocation routing(사용자 위치 기반), Failover routing(장애 시 전환).

> ⭐⭐⭐⭐ **Level 4 (리드/CTO)**
> - 글로벌 아키텍처 설계: 규정, 지연, 비용 균형
> - 데이터 주권: 국가별 데이터 저장 규정
>
> **Q: 글로벌 서비스 아키텍처 설계 시 고려사항은?**
> **A:** 사용자 분포, 지연 요구사항, 데이터 규정(GDPR 등), 운영 복잡도, 비용, 장애 격리, 배포 전략.
>
> **Q: 데이터 주권 규정 대응 방법은?**
> **A:** 리전별 데이터 격리, 데이터 분류 및 태깅, 복제 정책 제한, 감사 로그, 법무팀 협업.

---

## 실무 시나리오

### 시나리오 1: 대규모 장애 발생
**상황**: 프로덕션 DB 연결 풀 고갈로 전체 서비스 다운
**대응**:
1. IC 지정, 장애 선언 (SEV1)
2. 상태 페이지 업데이트, 이해관계자 알림
3. 즉시 완화: 연결 풀 크기 증가, DB 인스턴스 추가
4. 원인 분석: 쿼리 지연 유발 코드 식별
5. 근본 해결: 문제 쿼리 롤백 또는 수정
6. 포스트모텀: 연결 풀 모니터링, 알림 추가

### 시나리오 2: SLO 위반 추세
**상황**: 에러 버짓 50% 소진, 현재 추세로 주 내 소진 예상
**대응**:
1. 신규 기능 배포 보류 결정
2. 최근 배포 분석, 에러율 증가 원인 파악
3. 문제 배포 롤백 또는 수정
4. 안정화 확인 후 배포 재개
5. 에러 버짓 정책 리뷰, 팀과 학습 공유

### 시나리오 3: 카나리 분석 실패
**상황**: 카나리 배포 중 p99 지연시간 50% 증가 감지
**대응**:
1. 자동 롤백 또는 수동 롤백 실행
2. 변경 사항 분석: 새로운 API 호출? 리소스 경합?
3. 스테이징에서 재현 및 디버깅
4. 수정 후 더 작은 카나리 비율로 재시도
5. 성공 시 점진적 확대

---

## 면접 빈출 질문

**Q: SLI, SLO, SLA의 차이를 설명하고 좋은 SLO를 설정하는 방법은?**
**A:** SLI는 서비스 품질 측정 지표(가용성, 지연시간), SLO는 목표 수치(99.9%), SLA는 계약(위반 시 보상). 좋은 SLO는 사용자 경험 반영, 측정 가능, 달성 가능하면서 도전적, 의존성 고려, 에러 버짓으로 개발 속도와 균형.

**Q: 에러 버짓이란 무엇이고 어떻게 활용하는가?**
**A:** 100% - SLO = 에러 버짓. 허용 가능한 장애/에러의 양. 버짓 내에서 빠른 개발, 버짓 소진 시 안정화 우선. 개발팀과 운영팀의 객관적 협력 기준.

**Q: Blameless 포스트모텀이란?**
**A:** 개인을 비난하지 않고 시스템 개선에 집중하는 장애 분석 문화. "누가 잘못했나"가 아니라 "시스템이 왜 이런 실수를 허용했나" 질문. 심리적 안전감으로 솔직한 원인 분석과 학습 가능.

**Q: 온콜 번아웃을 방지하는 방법은?**
**A:** 충분한 로테이션 풀(최소 8명), 공정한 분배, 적정 보상, 알림 피로 감소(불필요 알림 제거, 자동화), 교대 후 휴식, 정기적인 피드백과 개선, 경영진의 온콜 중요성 인식.

**Q: RTO와 RPO를 기반으로 DR 전략을 어떻게 선택하는가?**
**A:** RTO 수 시간, RPO 수 시간: Backup/Restore(저비용). RTO 분 단위, RPO 분 단위: Warm Standby. RTO 초 단위, RPO 0: Multi-Site Active-Active(고비용). 비즈니스 요구와 비용 균형.

**Q: Chaos Engineering을 프로덕션에서 안전하게 수행하는 방법은?**
**A:** 1) Steady State 정의 2) 작은 범위로 시작 3) 자동 중단 조건 설정 4) 업무 시간 내 수행 5) 롤백 계획 준비 6) 모니터링 강화 7) 점진적으로 범위 확대 8) 결과 문서화 및 학습.
