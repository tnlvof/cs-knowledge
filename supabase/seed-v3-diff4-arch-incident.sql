-- CS Knowledge Quiz - Level 4 (리드/CTO급) 문제
-- Architecture 25문제 + Incident 25문제 = 총 50문제
-- difficulty=4, level_min=50, level_max=100
-- Generated: 2026-02-14

-- ============================================
-- ARCHITECTURE CATEGORY (25 questions)
-- 리드/CTO급 난이도: 대규모 시스템 설계, 전략적 의사결정, 트레이드오프 분석
-- ============================================

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'microservices', 4, 50, 100, 'DAU 500만 소셜 서비스의 모놀리스를 MSA로 전환하려 합니다. 전환 순서, 데이터 마이그레이션 전략, 조직 구조 변경을 포함한 종합 마이그레이션 전략을 수립하세요.', '1) 전환 순서: Strangler Fig 패턴으로 트래픽이 낮고 독립적인 서비스(알림, 검색)부터 분리. 핵심 도메인(피드, 메시징)은 마지막에 분리. 2) 데이터 마이그레이션: 이중 쓰기(Dual Write) → CDC로 점진적 전환 → 새 서비스로 읽기 전환 → 쓰기 전환 → 레거시 제거. 3) 조직: 역콘웨이 전략으로 바운디드 컨텍스트별 팀 재편, 각 팀에 SRE 임베디드. 4) 필수 선행조건: CI/CD, 서비스 디스커버리, 분산 트레이싱, 중앙 로깅 인프라 구축.', ARRAY['Strangler Fig', 'CDC', 'Dual Write', '역콘웨이', 'MSA 전환'], 'MSA 전환은 기술 변경보다 조직 문화와 인프라 준비가 더 중요하다. 빅뱅 전환은 실패 확률이 높으므로 점진적 접근이 필수다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'event-driven', 4, 50, 100, '글로벌 e-commerce 플랫폼(MAU 1억)의 주문-결제-재고-배송 시스템을 이벤트 기반 아키텍처로 설계하세요. 데이터 일관성, 보상 트랜잭션, 멱등성 보장 방안을 포함해 설명하세요.', '1) 이벤트 스토어: Kafka를 이벤트 백본으로 사용, 토픽별 파티션은 주문ID 기반 해싱으로 순서 보장. 2) Saga 패턴: Orchestration 방식으로 OrderSaga가 각 서비스 조율. 3) 보상 트랜잭션: 재고 차감 실패 시 결제 취소 이벤트 발행 → PG사 환불 API 호출. 4) 멱등성: 각 서비스에서 이벤트ID 기반 중복 처리 테이블 관리. 5) Outbox 패턴: 로컬 트랜잭션으로 이벤트 저장 후 Debezium CDC로 Kafka 발행. 6) 글로벌 대응: 리전별 Kafka 클러스터 + MirrorMaker2로 크로스 리전 복제.', ARRAY['Saga', 'Outbox', 'CDC', '멱등성', 'Kafka', '보상 트랜잭션'], '대규모 분산 시스템에서 강한 일관성 대신 결과적 일관성을 선택하되, 비즈니스 크리티컬 흐름에는 반드시 보상 로직이 필요하다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'cqrs', 4, 50, 100, '일 1억 건 이상의 거래가 발생하는 핀테크 플랫폼에서 CQRS + Event Sourcing을 도입하려 합니다. 감사(Audit) 요구사항, 시점 복구, 읽기 모델 재구축 전략을 설계하세요.', '1) 이벤트 스토어 설계: PostgreSQL + 불변 이벤트 테이블(id, aggregate_id, event_type, payload, timestamp, version). 2) 스냅샷 전략: 100개 이벤트마다 스냅샷 저장, 복구 시 최신 스냅샷 + 이후 이벤트 재생. 3) 읽기 모델: 실시간은 Redis, 분석용은 Elasticsearch, 보고서는 ClickHouse. 4) Projection 재구축: 새 버전 Projection은 별도 인스턴스에서 전체 이벤트 재생 → Blue-Green 전환. 5) 감사: 모든 이벤트에 actor_id, ip, user_agent 메타데이터 포함, 7년 보관 정책. 6) 시점 복구: 특정 시점까지의 이벤트만 재생하여 해당 시점 상태 재현.', ARRAY['Event Sourcing', 'CQRS', 'Snapshot', 'Projection', '감사 로그'], 'Event Sourcing은 완전한 감사 추적과 시간 여행 쿼리를 제공하지만, 이벤트 스키마 버전 관리와 읽기 모델 동기화 복잡성이 큰 트레이드오프다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'api-gateway', 4, 50, 100, '멀티 클라우드(AWS + GCP) 환경에서 글로벌 API Gateway 아키텍처를 설계하세요. 인증 통합, Rate Limiting 공유, 장애 시 자동 페일오버 전략을 포함하세요.', '1) 글로벌 라우팅: Cloudflare 또는 자체 GeoDNS로 가장 가까운 클라우드 리전으로 라우팅. 2) API Gateway: 각 클라우드에 Kong 또는 Envoy Gateway 배포, 설정은 GitOps로 동기화. 3) 인증 통합: 중앙 IdP(Okta/Auth0)에서 JWT 발급, 각 Gateway에서 공개키로 검증, 키 캐싱 + 주기적 갱신. 4) 분산 Rate Limiting: Redis Cluster를 멀티 리전으로 배포, Sliding Window 알고리즘으로 글로벌 쿼터 공유. 5) 페일오버: 헬스체크 실패 시 DNS TTL 30초 이내 자동 전환, 또는 Anycast IP로 BGP 기반 라우팅.', ARRAY['멀티 클라우드', 'API Gateway', 'Rate Limiting', '분산 인증', '페일오버'], '멀티 클라우드는 벤더 락인을 피하지만 운영 복잡도가 기하급수적으로 증가한다. 표준화된 도구와 GitOps가 필수다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'distributed-system', 4, 50, 100, '글로벌 실시간 협업 도구(Google Docs 유사)의 분산 아키텍처를 설계하세요. 동시 편집 충돌 해결, 지연 시간 최소화, 오프라인 지원 전략을 설명하세요.', '1) 충돌 해결: CRDT(Conflict-free Replicated Data Type) 또는 OT(Operational Transformation) 사용. CRDT가 더 단순하고 오프라인 지원에 유리. 2) 아키텍처: 리전별 협업 서버 배치, WebSocket으로 실시간 동기화. 3) 지연 최소화: 클라이언트 낙관적 업데이트 → 서버 확인 → 충돌 시 자동 병합. 4) 오프라인: IndexedDB에 로컬 변경 저장, 온라인 복귀 시 서버와 CRDT 병합. 5) 순서 보장: Vector Clock 또는 Lamport Timestamp로 인과 관계 추적. 6) 확장: 문서별로 샤딩, 핫 문서는 전용 서버 할당.', ARRAY['CRDT', 'OT', 'Vector Clock', '실시간 협업', 'WebSocket'], '실시간 협업 시스템의 핵심은 네트워크 지연과 오프라인 상황에서도 일관된 사용자 경험을 제공하는 것이다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'caching', 4, 50, 100, '초당 100만 요청을 처리하는 콘텐츠 플랫폼의 멀티 레이어 캐싱 아키텍처를 설계하세요. 캐시 일관성, 워밍, 장애 대응 전략을 포함하세요.', '1) L1 캐시: 애플리케이션 로컬 메모리(Caffeine/Guava), TTL 30초, 용량 제한으로 메모리 관리. 2) L2 캐시: Redis Cluster 6노드 이상, 리전별 배치, TTL 5분. 3) CDN: CloudFront/Fastly로 정적 콘텐츠, Edge에서 동적 콘텐츠 캐싱. 4) 일관성: 쓰기 시 Pub/Sub으로 L1 무효화 전파, L2는 Write-Through, CDN은 Purge API. 5) 워밍: 배포 전 인기 콘텐츠 미리 로딩, Canary 서버로 캐시 프리히팅. 6) 장애 대응: L2 장애 시 L1만으로 서비스 + 회로 차단기로 DB 보호, stale-while-revalidate 패턴.', ARRAY['멀티레이어 캐시', 'Cache Invalidation', '캐시 워밍', 'Redis Cluster', 'CDN'], '캐시 무효화는 컴퓨터 과학에서 가장 어려운 문제 중 하나다. 계층별로 다른 TTL과 무효화 전략을 조합해야 한다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'scalability', 4, 50, 100, '블랙프라이데이에 평소 대비 50배 트래픽이 예상되는 e-commerce 플랫폼의 용량 계획과 확장 전략을 수립하세요. 비용 최적화도 고려하세요.', '1) 용량 계획: 피크 TPS 예측(과거 데이터 + 마케팅 예측), N+2 버퍼 포함하여 산정. 2) 사전 확장: 이벤트 48시간 전 수동 스케일 아웃(Auto Scaling보다 빠름). 3) 비용 최적화: Reserved Instance로 베이스라인 70% 커버, Spot Instance로 버스트 트래픽 처리. 4) 캐시 극대화: 상품 정보 100% 캐시 적중률 목표, 실시간 재고만 DB 조회. 5) 비핵심 기능 비활성화: 추천, 리뷰, 개인화 등 비필수 기능 Circuit Breaker로 자동 차단. 6) Queue 기반 주문: 주문 접수만 빠르게 처리, 결제/재고 차감은 비동기 큐로 지연 처리. 7) 모니터링: 실시간 대시보드, 1분 단위 용량 경보.', ARRAY['용량 계획', '스케일 아웃', 'Spot Instance', '이벤트 대응', '비동기 처리'], '대규모 이벤트 대응은 기술보다 준비와 커뮤니케이션이 중요하다. 사전 부하 테스트와 게임데이 훈련이 필수다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'service-mesh', 4, 50, 100, '200개 이상의 마이크로서비스를 운영하는 조직에서 서비스 메시 도입을 검토 중입니다. Istio vs Linkerd 선택 기준, 점진적 도입 전략, 성능 오버헤드 관리 방안을 제시하세요.', '1) 선택 기준: Istio는 기능 풍부(정교한 트래픽 관리, Wasm 확장), 복잡도 높음. Linkerd는 경량/단순, Rust 기반으로 메모리 효율적. 200+ 서비스면 Linkerd의 단순함 권장. 2) 점진적 도입: 1단계-관측성만(사이드카 인젝션, 메트릭 수집) → 2단계-mTLS 활성화 → 3단계-트래픽 관리(카나리, 재시도) → 4단계-정책(Rate Limiting). 3) 성능 관리: 각 단계에서 p99 지연 시간 측정, 5ms 이상 증가 시 튜닝. 4) 예외 처리: 지연 민감 서비스는 사이드카 없이 운영, 점진적 마이그레이션. 5) 모니터링: Kiali + Grafana 대시보드, 서비스 토폴로지 시각화.', ARRAY['Service Mesh', 'Istio', 'Linkerd', 'mTLS', '사이드카'], '서비스 메시는 강력하지만 모든 팀이 Kubernetes와 네트워킹에 익숙해야 효과적이다. 조직 역량을 먼저 평가하라.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'ddd', 4, 50, 100, '레거시 ERP 시스템과 신규 MSA 플랫폼을 통합해야 합니다. DDD 관점에서 Bounded Context 간 통합 패턴과 Anti-corruption Layer 설계 전략을 설명하세요.', '1) Context Mapping: 레거시 ERP는 Big Ball of Mud로 인식, 신규 MSA의 각 Bounded Context와 명확한 경계 설정. 2) 통합 패턴 선택: Customer-Supplier(레거시가 Upstream, 신규가 Downstream) 또는 Conformist(레거시 모델 그대로 수용). 3) ACL 설계: 전용 Adapter 서비스 배치, 레거시 API 호출 → 내부 도메인 모델로 변환. 4) 데이터 동기화: CDC로 레거시 DB 변경 감지 → 도메인 이벤트 변환 → 신규 서비스 소비. 5) 점진적 대체: 새 기능은 신규 서비스에서 개발, 레거시 기능은 ACL 통해 호출, 장기적으로 레거시 폐기. 6) 팀 구조: 레거시 전담팀과 신규 서비스팀 분리, ACL 서비스 공동 소유.', ARRAY['DDD', 'Bounded Context', 'ACL', 'Context Mapping', '레거시 통합'], 'ACL은 레거시의 기술 부채가 신규 시스템으로 전파되는 것을 방지하는 중요한 보호막이다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'cap-theorem', 4, 50, 100, '금융 거래 시스템에서 계좌 이체 기능을 설계할 때, CAP 정리와 PACELC를 고려한 일관성 전략을 수립하세요. 네트워크 파티션 시나리오별 대응 방안을 포함하세요.', '1) 요구사항 분석: 계좌 이체는 돈이 관련되므로 일관성(C)이 가용성(A)보다 우선. CP 시스템 선택. 2) 정상 상태(PACELC-C): 동기 복제로 강한 일관성 유지, 약간의 지연 감수. 3) 파티션 시나리오: a) DC간 파티션 시 쓰기 거부(409 Conflict), 읽기는 허용 b) 단일 DC 장애 시 나머지 DC의 쿼럼으로 서비스 지속. 4) 구현: CockroachDB 또는 Spanner 같은 분산 SQL DB, Raft 기반 합의. 5) 보상: 파티션 복구 후 자동 데이터 재조정, 충돌 시 관리자 개입 필요 케이스 알림. 6) 사용자 경험: 파티션 중 ''일시적 서비스 불가'' 메시지, 가능하면 읽기 전용 모드 제공.', ARRAY['CAP', 'PACELC', '강한 일관성', '분산 트랜잭션', '금융 시스템'], '금융 시스템에서 잘못된 잔액보다 일시적 서비스 불가가 낫다. 비즈니스 요구사항에 따라 CAP 트레이드오프를 결정해야 한다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'serverless', 4, 50, 100, '서버리스 아키텍처로 실시간 데이터 파이프라인을 구축하려 합니다. 초당 10만 이벤트 처리, 비용 예측, 콜드 스타트 회피 전략을 포함한 설계를 제시하세요.', '1) 아키텍처: Kinesis Data Streams → Lambda Consumer → DynamoDB/S3. 2) Lambda 설정: Provisioned Concurrency 500으로 콜드 스타트 제거, 배치 사이즈 100, 최대 6시간 실행 제한 고려. 3) 비용 예측: 월 2.6억 이벤트 × Lambda 실행 비용 + Kinesis 샤드 비용 + Provisioned Concurrency 비용. EC2 상시 운영 대비 ROI 분석 필수. 4) 대안: 트래픽이 일정하면 ECS/Fargate가 더 저렴할 수 있음. 5) 처리량 확보: Kinesis Enhanced Fan-Out으로 샤드당 처리량 2MB/s 보장. 6) 에러 처리: DLQ로 실패 이벤트 격리, CloudWatch Alarm으로 DLQ 깊이 모니터링. 7) 관측성: X-Ray로 분산 트레이싱, 커스텀 메트릭으로 처리 지연 측정.', ARRAY['Serverless', 'Lambda', 'Kinesis', 'Provisioned Concurrency', '이벤트 파이프라인'], '서버리스는 항상 저렴하지 않다. 일정한 고트래픽에서는 컨테이너가 더 비용 효율적일 수 있다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'message-queue', 4, 50, 100, 'Kafka 클러스터를 글로벌 규모로 운영 중입니다. 리전 간 복제 전략, 파티션 재조정 시 무중단 방안, 장기 데이터 보존 정책을 설계하세요.', '1) 리전 간 복제: MirrorMaker2로 Active-Passive 복제, 또는 Confluent Cluster Linking으로 Active-Active. RPO 목표에 따라 동기화 지연 모니터링. 2) 파티션 재조정: Cruise Control로 자동화, 재조정 시 스로틀링으로 프로듀서/컨슈머 영향 최소화, ISR(In-Sync Replicas) 유지 확인. 3) 무중단 업그레이드: Rolling Restart, 브로커 한 대씩 교체, Controlled Shutdown 활성화. 4) 장기 보존: Tiered Storage로 오래된 세그먼트 S3로 오프로드, Hot/Cold/Archive 티어 정책. 5) 모니터링: 컨슈머 랙, 언더 리플리케이티드 파티션, 요청 지연 시간 대시보드. 6) 보안: mTLS로 브로커 간 암호화, ACL로 토픽별 접근 제어.', ARRAY['Kafka', 'MirrorMaker2', 'Tiered Storage', 'Cruise Control', '글로벌 복제'], '대규모 Kafka 운영의 핵심은 파티션 밸런싱과 컨슈머 랙 관리다. 자동화 도구 없이는 운영이 불가능하다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'resilience', 4, 50, 100, '결제 시스템의 회복 탄력성을 강화하려 합니다. Circuit Breaker, Bulkhead, Retry, Timeout을 조합한 종합 장애 격리 전략을 설계하세요.', '1) Timeout: 외부 PG 연동 3초, 내부 서비스 1초로 설정. 무한 대기 방지가 모든 패턴의 기초. 2) Retry: 일시적 오류(5xx, 네트워크 타임아웃)만 재시도, 지수 백오프(100ms → 200ms → 400ms), 최대 3회. 멱등 보장된 API만 재시도. 3) Circuit Breaker: 10초 윈도우에서 50% 이상 실패 시 Open, 30초 후 Half-Open에서 3개 요청 테스트. 4) Bulkhead: PG사별 Thread Pool 분리(A사 20, B사 20), 한 PG 장애가 다른 PG에 영향 방지. 5) Fallback: 주 PG 장애 시 대체 PG로 자동 전환, 모든 PG 장애 시 ''나중에 결제'' 옵션 제공. 6) 적용 순서: Timeout → Retry → Circuit Breaker → Bulkhead (안에서 밖으로).', ARRAY['Circuit Breaker', 'Bulkhead', 'Retry', 'Timeout', '장애 격리'], '회복 탄력성 패턴은 단독보다 조합으로 사용할 때 효과적이다. 적용 순서와 설정값 튜닝이 중요하다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'data-architecture', 4, 50, 100, '실시간 분석과 배치 분석을 모두 지원하는 Lambda Architecture 대신 Kappa Architecture를 선택하는 기준과 구현 전략을 설명하세요.', '1) 선택 기준: 배치와 스트리밍 로직이 중복되어 유지보수 비용이 높다면 Kappa 고려. 스트리밍만으로 과거 데이터 재처리가 가능하다면 Kappa가 단순함. 2) Kappa 구현: Kafka를 이벤트 소스로, 장기 보존(수개월~수년) 설정. 분석 로직은 Kafka Streams 또는 Flink로 단일화. 3) 재처리: 새 로직 배포 시 새 Consumer Group으로 처음부터 재처리, 기존 결과와 병렬로 비교 후 전환. 4) 저장소: 실시간 뷰는 Redis/Elasticsearch, 분석 결과는 ClickHouse/Druid. 5) Lambda가 여전히 필요한 경우: 복잡한 ML 모델 학습, 대용량 Join 연산, 정확도가 최우선인 금융 보고서. 6) 하이브리드: 대부분 Kappa, 특수 케이스만 배치 잡으로 보완.', ARRAY['Lambda Architecture', 'Kappa Architecture', 'Kafka Streams', 'Flink', '스트리밍 처리'], '아키텍처 선택은 팀의 역량과 비즈니스 요구사항에 따라 달라진다. 단순함을 추구하되 필요시 복잡성을 수용하라.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'database-scaling', 4, 50, 100, '단일 PostgreSQL이 한계에 도달했습니다. 읽기 복제본, Citus(분산 확장), 샤딩 전략 중 선택 기준과 각각의 마이그레이션 계획을 수립하세요.', '1) 읽기 복제본: 읽기 80% 이상, 쓰기 부하 낮을 때. 마이그레이션: Streaming Replication 설정, 애플리케이션 읽기 쿼리 라우팅 변경. 가장 단순하고 빠름. 2) Citus: 단일 대형 테이블이 병목, 분석 쿼리 많을 때. 마이그레이션: Coordinator + Worker 노드 구성, 대상 테이블 분산 테이블로 변환, 애플리케이션 변경 최소화. 3) 수동 샤딩: 테넌트별/지역별 명확한 분리 가능할 때. 마이그레이션: 샤딩 키 결정 → 샤드별 DB 인스턴스 생성 → 데이터 분할 이동 → 라우팅 레이어 구현. 가장 복잡하지만 유연함. 4) 공통 고려사항: 크로스 샤드 조인/트랜잭션 최소화, ID 생성 전략(Snowflake ID), 샤드 재조정 자동화. 5) 권장: 읽기 복제본 먼저 시도, 부족하면 Citus, 마지막에 수동 샤딩.', ARRAY['PostgreSQL', 'Citus', '샤딩', '읽기 복제본', '데이터베이스 확장'], '데이터베이스 확장은 되돌리기 어렵다. 가장 단순한 방법부터 시도하고 점진적으로 복잡한 전략을 도입하라.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', '12-factor', 4, 50, 100, '15-Factor App 또는 Beyond 12-Factor 관점에서 현대 클라우드 네이티브 애플리케이션에 추가로 고려해야 할 원칙들을 설명하고, 기존 시스템에 적용하는 전략을 제시하세요.', '1) 추가 원칙: API First(인터페이스 우선 설계), Telemetry(메트릭/로그/트레이스 내장), Security(시크릿 관리, mTLS), Audit(변경 추적), Graceful Degradation(부분 장애 허용). 2) API First 적용: OpenAPI 스펙 먼저 작성, Mock 서버로 프론트엔드 개발 병행, Contract Testing 도입. 3) Telemetry 적용: OpenTelemetry SDK 통합, 표준 메타데이터(service.name, environment) 주입, 중앙 수집 플랫폼 구축. 4) Security 적용: Vault로 시크릿 중앙화, 애플리케이션 시작 시 동적 시크릿 주입, 만료/순환 자동화. 5) 레거시 전환: 점진적 적용, 새 기능부터 원칙 준수, 기존 코드는 리팩토링 시 개선. 6) 검증: Production Readiness Checklist에 각 원칙 포함, PR 리뷰에서 확인.', ARRAY['12-Factor', '15-Factor', 'API First', 'Telemetry', '클라우드 네이티브'], '12-Factor는 2011년에 작성되었다. 현대 환경에서는 보안, 관측성, API 설계 원칙이 추가로 중요해졌다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'edge-computing', 4, 50, 100, 'IoT 기반 스마트 팩토리(공장 1000개, 센서 100만개)의 엣지-클라우드 하이브리드 아키텍처를 설계하세요. 데이터 흐름, 지연 요구사항, 오프라인 운영을 고려하세요.', '1) 엣지 계층(공장별): 산업용 게이트웨이(ARM 기반)에서 센서 데이터 수집, 로컬 필터링/집계, 이상 탐지 ML 모델 실행(TensorFlow Lite). 2) 지연 요구사항: 긴급 알림(장비 이상)은 50ms 내 로컬 처리, 품질 분석은 1초 내 엣지에서 처리. 3) 오프라인 운영: 엣지에서 24시간치 데이터 버퍼링, 연결 복구 시 배치 업로드. 4) 클라우드 계층: 집계된 데이터 수신, 장기 저장(S3 + Athena), 전체 공장 대시보드, 중앙 ML 모델 학습 후 엣지 배포. 5) 보안: 엣지-클라우드 간 mTLS, 디바이스 인증서 기반 인증, OTA 업데이트 서명 검증. 6) 관리: 중앙에서 엣지 디바이스 Fleet 관리(AWS IoT Greengrass, Azure IoT Edge).', ARRAY['Edge Computing', 'IoT', '스마트 팩토리', '하이브리드 아키텍처', 'TensorFlow Lite'], '엣지 컴퓨팅의 핵심은 ''무엇을 엣지에서 처리하고 무엇을 클라우드로 보낼지'' 결정하는 것이다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'zero-trust', 4, 50, 100, '기존 VPN 기반 네트워크 보안에서 Zero Trust Architecture로 전환하려 합니다. BeyondCorp 모델 기반의 단계적 전환 전략과 필요한 기술 스택을 설명하세요.', '1) Zero Trust 원칙: 네트워크 위치가 아닌 신원과 컨텍스트 기반 접근 제어. ''신뢰하지 않고, 항상 검증''. 2) 1단계-신원 중앙화: IdP(Okta/Azure AD)로 통합 SSO, MFA 필수화, 디바이스 인증서 발급. 3) 2단계-접근 프록시: BeyondCorp Enterprise 또는 Cloudflare Access로 앱별 접근 프록시 배치, VPN 없이 인터넷을 통해 접근. 4) 3단계-마이크로 세그멘테이션: 서비스 간 mTLS, 네트워크 정책으로 최소 권한 원칙. 5) 4단계-지속적 검증: 세션 중에도 디바이스 상태(패치, 보안 SW) 지속 확인, 이상 시 접근 차단. 6) VPN 폐기: 마지막 애플리케이션 마이그레이션 후 VPN 단계적 제거. 7) 레거시 대응: 프록시가 불가한 레거시는 별도 점프 호스트 경유.', ARRAY['Zero Trust', 'BeyondCorp', 'mTLS', 'ZTNA', 'VPN 대체'], 'Zero Trust 전환은 기술보다 문화 변화가 어렵다. 사용자 경험을 해치지 않으면서 보안을 강화해야 한다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'platform-engineering', 4, 50, 100, '개발팀 50개, 서비스 300개를 지원하는 내부 개발자 플랫폼(IDP)을 설계하세요. 셀프 서비스, 가드레일, 표준화 전략을 포함하세요.', '1) 셀프 서비스 카탈로그: Backstage 기반 IDP 포털, 서비스 생성/배포/모니터링 원클릭 제공. 2) Golden Path: 표준 템플릿(언어별, 서비스 유형별) 제공, Cookiecutter 또는 Yeoman으로 프로젝트 스캐폴딩. 3) 가드레일: OPA(Open Policy Agent)로 정책 강제, 비준수 배포 차단. 예: 리소스 제한 필수, 보안 스캔 통과 필수. 4) 표준화: CI/CD는 GitHub Actions + ArgoCD 표준 파이프라인, 로깅/메트릭/트레이싱 자동 설정. 5) 추상화 레벨: 개발자는 Kubernetes 직접 접근 대신 Helm/Kustomize 템플릿만 수정. 6) 피드백 루프: 플랫폼 팀-개발팀 정기 싱크, NPS 조사, 기능 요청 백로그 관리. 7) 온보딩: 신규 팀 2시간 내 첫 서비스 배포 가능 목표.', ARRAY['Platform Engineering', 'IDP', 'Backstage', 'Golden Path', 'OPA'], '플랫폼 엔지니어링의 성공 지표는 개발자 생산성 향상이다. 플랫폼은 제품처럼 관리되어야 한다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'multi-tenancy', 4, 50, 100, 'B2B SaaS 플랫폼의 멀티 테넌시 아키텍처를 설계하세요. 데이터 격리 수준, 테넌트별 커스터마이징, 노이지 네이버(Noisy Neighbor) 방지 전략을 포함하세요.', '1) 격리 수준 선택: Silo(테넌트별 DB) vs Pool(공유 DB+테넌트 ID 컬럼) vs Bridge(스키마 분리). 대기업 고객은 Silo, 중소 고객은 Pool. 2) 데이터 격리: Row-Level Security(RLS)로 쿼리에 자동 테넌트 필터 적용, 실수로 타 테넌트 데이터 노출 방지. 3) 커스터마이징: 설정 기반(로고, 색상), 기능 플래그(모듈 활성화), 웹훅(워크플로 확장). 코드 포크 지양. 4) Noisy Neighbor 방지: 테넌트별 Rate Limiting, 리소스 쿼터(CPU/메모리), 대형 고객은 전용 노드 풀. 5) 확장: 테넌트 ID 기반 샤딩, 테넌트 메타데이터 캐싱, 대형 테넌트 별도 클러스터 이전 자동화. 6) 모니터링: 테넌트별 메트릭 분리, 사용량 대시보드, 빌링 연동.', ARRAY['Multi-tenancy', 'SaaS', 'Noisy Neighbor', 'RLS', '테넌트 격리'], '멀티 테넌시 설계 초기 결정은 나중에 변경하기 매우 어렵다. 최대 테넌트 규모를 예측하고 설계해야 한다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'search-architecture', 4, 50, 100, '매일 1억 개의 문서가 생성되는 플랫폼의 검색 아키텍처를 설계하세요. 인덱싱 전략, 검색 품질, 운영 확장성을 고려하세요.', '1) 검색 엔진: Elasticsearch 클러스터, 핫/웜/콜드 티어로 비용 최적화. 2) 인덱싱 전략: 일별 인덱스(logs-2024.02.14), 자동 롤오버, 오래된 인덱스 웜/콜드로 이동. 3) 실시간 인덱싱: Kafka → Logstash/Vector → ES, 벌크 인덱싱으로 처리량 확보. 4) 검색 품질: BM25 기본, 필드별 부스팅, 동의어 사전, 오타 교정(fuzzy), 사용자 피드백 기반 랭킹 튜닝. 5) 성능 최적화: 검색 결과 캐싱(Redis), 필터 쿼리 캐싱, 페이지네이션은 search_after 사용. 6) 클러스터 관리: 전용 마스터 노드 3개, 데이터 노드 샤드 밸런싱, Cross-Cluster Replication으로 DR. 7) 대안: 특정 유스케이스에 OpenSearch, Meilisearch, Typesense 검토.', ARRAY['Elasticsearch', '검색 아키텍처', '인덱싱', 'BM25', '실시간 검색'], '대규모 검색 시스템의 핵심은 인덱싱 처리량과 검색 지연 시간의 균형이다. 샤드 설계가 확장성을 결정한다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'api-versioning', 4, 50, 100, '외부 파트너 100개사가 사용하는 Public API의 버전 관리 및 폐기(Deprecation) 전략을 수립하세요. 하위 호환성 유지와 마이그레이션 지원 방안을 포함하세요.', '1) 버전 관리 방식: URL 경로(/v1/, /v2/)로 명시적 버전 관리, 시맨틱 버저닝 적용. 2) 하위 호환성 규칙: 필드 추가는 OK, 필드 제거/타입 변경은 새 버전. Postel''s Law(관대하게 수용, 엄격하게 전송) 적용. 3) Deprecation 정책: 최소 12개월 사전 공지, Sunset 헤더로 폐기 일정 전달, 이메일/대시보드 알림. 4) 마이그레이션 지원: 버전별 마이그레이션 가이드 제공, SDK 자동 업데이트, 샌드박스 환경에서 새 버전 테스트 지원. 5) 모니터링: 버전별 사용량 추적, 폐기 예정 버전 사용 파트너 개별 연락. 6) 비용: 동시에 최대 3개 버전 유지(N, N-1, N-2), 그 이상은 폐기. 7) 거버넌스: API 변경 RFC 프로세스, 파트너 영향도 평가 필수.', ARRAY['API Versioning', 'Deprecation', '하위 호환성', 'Semantic Versioning', 'Public API'], 'Public API는 한번 공개하면 되돌리기 어렵다. 처음부터 확장 가능한 설계와 명확한 폐기 정책이 필요하다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'ml-serving', 4, 50, 100, '실시간 ML 추론 서비스(초당 10만 요청)의 서빙 아키텍처를 설계하세요. 모델 버전 관리, A/B 테스트, 롤백 전략을 포함하세요.', '1) 서빙 인프라: KServe/Seldon Core 또는 TensorFlow Serving on Kubernetes, GPU 노드 풀 별도 구성. 2) 모델 버전 관리: MLflow Model Registry, 각 모델 버전별 아티팩트와 메트릭 저장. 3) 배포 전략: Canary 배포로 새 모델 10% 트래픽 시작, 비즈니스 메트릭(CTR, 전환율) 비교 후 점진적 확대. 4) A/B 테스트: Feature Flag 서비스와 연동, 사용자 ID 기반 일관된 할당, 통계적 유의성 검증 자동화. 5) 롤백: 메트릭 악화 시 즉시 이전 버전으로 자동 롤백, 모델 성능 SLO 정의(지연 p99 < 50ms, 에러율 < 0.1%). 6) 캐싱: 동일 입력에 대한 추론 결과 Redis 캐싱, 특히 추천 시스템에서 효과적. 7) 모니터링: 모델 드리프트 탐지, 입력 분포 변화 알림, 주기적 재학습 파이프라인.', ARRAY['ML Serving', 'KServe', 'Model Registry', 'A/B 테스트', '카나리 배포'], 'ML 시스템의 품질은 모델 정확도만이 아니라 안정적인 서빙과 빠른 실험 사이클에서 나온다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'cost-optimization', 4, 50, 100, '월 클라우드 비용 10억원인 조직에서 FinOps 관점의 비용 최적화 전략을 수립하세요. 가시성, 최적화, 거버넌스를 포함하세요.', '1) 가시성 확보: 모든 리소스 태깅 의무화(서비스명, 팀명, 환경), 팀별 비용 할당 대시보드(AWS Cost Explorer, Datadog Cost), 월간 비용 리뷰 미팅. 2) 인스턴스 최적화: Right-sizing 분석(CloudWatch 기반), 유휴 리소스 자동 감지 및 제거, 개발/스테이징 환경 야간 종료. 3) 가격 모델 최적화: 안정 워크로드는 Savings Plans/Reserved Instance 70% 커버, 변동 워크로드는 Spot Instance, 서버리스 전환 검토. 4) 아키텍처 최적화: 캐시 적중률 향상, 데이터 전송 비용 분석(AZ 간, 리전 간), 압축/중복 제거. 5) 거버넌스: 예산 알림, 팀별 비용 책임제, 신규 리소스 비용 예측 필수화. 6) 문화: 엔지니어 비용 인식 교육, 비용 절감 성과 인정.', ARRAY['FinOps', '비용 최적화', 'Right-sizing', 'Savings Plans', '클라우드 비용'], 'FinOps는 기술이 아니라 문화다. 엔지니어가 비용을 의식하고 의사결정에 반영해야 효과가 있다.', 'docs/07-architecture-scalability.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('architecture', 'observability-architecture', 4, 50, 100, '500개 서비스, 초당 100만 이벤트를 처리하는 관측성 플랫폼 아키텍처를 설계하세요. 메트릭, 로그, 트레이스 통합 전략과 비용 관리를 포함하세요.', '1) 메트릭: Prometheus Federation 또는 Thanos/Cortex로 장기 저장, 서비스별 메트릭 카디널리티 제한. 2) 로그: Vector/Fluent Bit → Kafka → Loki/Elasticsearch. 샘플링과 로그 레벨 동적 조정으로 볼륨 관리. 3) 트레이스: OpenTelemetry Collector → Jaeger/Tempo. Tail-based Sampling으로 에러/지연 트레이스만 100% 저장. 4) 통합: Grafana로 단일 대시보드, Trace ID로 메트릭-로그-트레이스 연결. 5) 비용 관리: 티어별 보존 정책(핫 7일, 웜 30일, 콜드 1년), 불필요한 로그 필터링, 메트릭 다운샘플링. 6) 알림: SLO 기반 알림, Alertmanager로 중복 제거 및 그룹화, 온콜 통합. 7) 셀프 서비스: 팀별 대시보드 템플릿, 알림 생성 권한 위임.', ARRAY['Observability', 'Prometheus', 'Loki', 'OpenTelemetry', '관측성 플랫폼'], '관측성 플랫폼은 조직의 규모에 따라 기하급수적으로 데이터가 증가한다. 초기부터 비용을 고려한 설계가 필수다.', 'docs/07-architecture-scalability.md');


-- ============================================
-- INCIDENT CATEGORY (25 questions)
-- 리드/CTO급 난이도: 글로벌 장애 대응, 포스트모텀, 카오스 엔지니어링, SRE 조직 운영
-- ============================================

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'incident-response', 4, 50, 100, '글로벌 서비스에서 SEV1 장애가 발생했습니다. Incident Commander로서 최초 10분 동안 수행해야 할 대응 프로세스를 수립하세요.', '1) 0-2분: 장애 선언, Slack/Teams 전용 채널 생성, IC 역할 명시적 선언. 2) 2-5분: Communications Lead, Scribe 역할 지정, SME(Subject Matter Expert) 호출. 3) 5-10분: 영향 범위 파악(영향받는 사용자 수, 지역, 기능), 상태 페이지 초기 업데이트, 경영진 알림. 4) 병렬 진행: 모니터링 대시보드로 문제 범위 확인, 최근 배포/변경 이력 확인, 롤백 가능 여부 판단. 5) 의사결정: 롤백할지 핫픽스할지 결정, 불확실하면 롤백 우선. 6) IC 원칙: 직접 디버깅하지 않음, 전체 상황 파악에 집중, 모든 행동 기록 지시.', ARRAY['Incident Commander', 'SEV1', '장애 대응', 'IC 역할', '초기 대응'], 'IC의 핵심 역할은 조율과 의사결정이다. 직접 문제를 해결하려 하면 전체 상황을 놓친다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'postmortem', 4, 50, 100, '대규모 장애 후 Blameless Postmortem을 작성하려 합니다. 포스트모텀의 필수 구성 요소, 5 Whys를 넘어선 시스템적 원인 분석, 액션 아이템 추적 체계를 설계하세요.', '1) 필수 구성: 요약(영향도, 지속시간, 복구시간), 상세 타임라인, 근본 원인 분석, 기여 요인들, 영향 분석(사용자/매출), 액션 아이템, 학습 사항. 2) 시스템적 분석: 5 Whys + Fault Tree Analysis로 다중 원인 분석. ''사람이 실수했다''에서 멈추지 않고 ''시스템이 왜 실수를 허용했나'' 질문. 3) 기여 요인: 직접 원인 외에 기여한 요소들(코드 리뷰 부재, 모니터링 갭, 테스트 부족) 모두 기록. 4) 액션 아이템: SMART 원칙(구체적, 측정 가능, 담당자, 기한), 우선순위(P0/P1/P2). 5) 추적 체계: Jira 에픽으로 관리, 주간 리뷰 미팅, 완료율 KPI 추적. 6) 공유: 전사 공유 세션, 요약본 위키 게시, 유사 장애 방지 체크리스트 업데이트.', ARRAY['Postmortem', 'Blameless', '5 Whys', 'Fault Tree', '액션 아이템'], 'Blameless의 핵심은 심리적 안전이다. 비난하면 문제를 숨기게 되고, 진짜 원인을 찾을 수 없다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'chaos-engineering', 4, 50, 100, '프로덕션 환경에서 카오스 엔지니어링을 도입하려 합니다. GameDay 시나리오 설계, 안전한 실험 실행, 조직 문화 정착 전략을 제시하세요.', '1) 성숙도 평가: 스테이징에서 먼저 실험, 충분한 관측성 확보 확인, 롤백 메커니즘 검증. 2) GameDay 시나리오: a) 단일 AZ 장애 시뮬레이션 b) 핵심 의존성(DB, 캐시) 지연/장애 c) 트래픽 10배 급증 d) DNS 장애. 3) 실험 설계: 가설 명시(예: ''Redis 장애 시 캐시 우회하여 서비스 지속''), Steady State 정의(에러율, 지연), Blast Radius 제한(특정 리전/사용자). 4) 안전 장치: Kill Switch로 즉시 중단, 업무 시간 내 실행, 영향 범위 자동 모니터링. 5) 도구: Gremlin, Litmus, AWS FIS 중 선택. 6) 문화 정착: 경영진 지지 확보, 성공 사례 공유, 장애 발견을 칭찬(비난 X), 분기별 정기 GameDay 일정화.', ARRAY['Chaos Engineering', 'GameDay', 'Blast Radius', 'Steady State', 'Gremlin'], '카오스 엔지니어링의 목표는 장애를 일으키는 것이 아니라, 약점을 발견하고 개선하는 것이다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'slo-sla', 4, 50, 100, '엔터프라이즈 고객과 SLA를 협상해야 합니다. SLI 선정, SLO 설정, SLA 보상 조건, 내부 운영 버퍼 전략을 수립하세요.', '1) SLI 선정: 고객 경험 반영 지표 선택. 가용성(성공 요청/전체 요청), 지연(p99 응답시간), 에러율. 내부 지표가 아닌 고객 관점 지표. 2) SLO 설정: 내부 SLO 99.95%, 외부 SLA 99.9%로 버퍼 확보. SLO가 SLA보다 엄격해야 페널티 전에 대응 가능. 3) 측정 방법: 합의된 측정 포인트(고객 측 vs 서버 측), 측정 도구, 보고 주기(월간) 명시. 4) 예외 조항: 계획된 유지보수, 고객 측 문제, 불가항력은 제외. 5) 보상 구조: 99.9% 미달 시 10% 크레딧, 99.5% 미달 시 25%, 99.0% 미달 시 50%. 상한선 설정. 6) 운영: 에러 버짓 대시보드 실시간 공유, 월간 SLA 리포트 자동 생성. 7) 협상 팁: 너무 공격적인 SLA 약속 지양, 달성 불가능한 SLA는 신뢰 손상.', ARRAY['SLA', 'SLO', 'SLI', '에러 버짓', 'SLA 협상'], 'SLA는 법적 계약이다. 내부 SLO보다 완화된 SLA를 설정하여 운영 여유를 확보해야 한다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'disaster-recovery', 4, 50, 100, 'RTO 15분, RPO 0인 금융 시스템의 재해 복구 아키텍처를 설계하세요. 페일오버 자동화, 데이터 정합성 검증, 정기 훈련 계획을 포함하세요.', '1) 아키텍처: Active-Active 멀티 리전, 동기 복제로 RPO 0 달성. Global Load Balancer로 자동 트래픽 전환. 2) 데이터 계층: Aurora Global Database 또는 CockroachDB로 리전 간 동기 복제, 복제 지연 실시간 모니터링. 3) 페일오버 자동화: Route 53 Health Check + Auto Failover, 또는 자체 헬스체크 서비스로 30초 내 전환. 4) 정합성 검증: 페일오버 후 자동 데이터 검증 잡 실행, 불일치 발견 시 알림. 5) 훈련 계획: 분기별 DR 훈련, 연 1회 실제 프로덕션 페일오버 테스트(고객 공지 후). 6) 문서화: DR 런북 상세 작성, 역할별 체크리스트, 연락망. 7) 감사: DR 테스트 결과 기록 보관, 규제 감사 대응.', ARRAY['RTO', 'RPO', 'DR', 'Active-Active', '페일오버'], 'RPO 0은 동기 복제가 필수이며, 이는 지연 시간 증가를 의미한다. 비즈니스 요구사항과 트레이드오프를 명확히 해야 한다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'on-call', 4, 50, 100, '글로벌 팀(3개 시간대)의 지속 가능한 온콜 운영 체계를 설계하세요. Follow-the-Sun 모델, 보상 정책, 번아웃 방지 전략을 포함하세요.', '1) Follow-the-Sun: 3개 리전(APAC, EMEA, Americas) 팀이 8시간씩 온콜 커버. 야간 온콜 제거. 2) 팀 규모: 리전당 최소 8명으로 주 1회 이하 온콜, 연속 온콜 금지. 3) 핸드오프: 리전 교대 시 30분 오버랩, 진행 중 이슈 브리핑, 비동기 문서화 필수. 4) 보상: 온콜 수당(일당), 호출 건당 추가 수당, 주말/공휴일 2배, 대체 휴무. 5) 번아웃 방지: 온콜 후 의무 휴식일, 분기별 온콜 부담 리뷰, 알림 수 KPI 추적 및 개선. 6) 알림 품질: 액션 가능한 알림만, 정보성 알림은 티켓으로, 주간 알림 리뷰로 노이즈 제거. 7) 도구: PagerDuty/Opsgenie로 스케줄 관리, 에스컬레이션 정책 자동화.', ARRAY['On-call', 'Follow-the-Sun', '온콜 보상', '번아웃', '글로벌 운영'], '지속 가능한 온콜의 핵심은 공정한 분배와 적절한 보상이다. 번아웃은 서비스 품질 저하로 이어진다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'error-budget', 4, 50, 100, '에러 버짓이 80% 소진된 상황에서 개발팀과 협의해야 합니다. 에러 버짓 정책 적용, 개발팀 설득, 장기적 신뢰성 문화 구축 전략을 제시하세요.', '1) 현황 공유: 에러 버짓 대시보드로 데이터 기반 논의, 감정적 접근 지양. 2) 정책 적용: 사전 합의된 정책 상기(75%: 신규 기능 배포 검토 강화, 100%: 기능 동결). 3) 영향 분석: 최근 장애 원인 분석 결과 공유, 어떤 변경이 버짓을 소진했는지 구체화. 4) 협력적 해결: 개발팀과 함께 안정화 우선순위 정하기, SRE가 지원할 영역 명확화. 5) 트레이드오프 설명: ''지금 안정화하면 다음 분기 더 빠르게 배포 가능'', 버짓 복구 후 배포 재개 약속. 6) 장기 문화: 정기 SLO 리뷰 미팅, 에러 버짓을 팀 KPI에 포함, 신뢰성을 공동 목표로 설정. 7) 에스컬레이션: 합의 불가 시 경영진 판단 요청, 비즈니스 임팩트 정량화.', ARRAY['에러 버짓', 'Error Budget Policy', 'SRE 협업', '신뢰성 문화'], '에러 버짓은 갈등의 도구가 아니라 객관적 의사결정 기준이다. 양팀이 공동 소유해야 효과적이다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'runbook', 4, 50, 100, '100개 이상의 런북을 관리하는 조직에서 런북 품질 관리, 자동화 연계, 지속적 업데이트 체계를 설계하세요.', '1) 런북 표준화: 템플릿 강제(증상, 영향도, 진단 단계, 조치 단계, 에스컬레이션, 관련 링크). 2) 알림 연계: 각 알림에 런북 링크 자동 포함, PagerDuty 어노테이션 또는 Slack 알림 내 링크. 3) 자동화: 진단 단계 스크립트화(kubectl, AWS CLI), 복구 조치 원클릭 자동화, ChatOps 통합. 4) 품질 관리: 장애 대응 후 런북 유효성 체크, 분기별 전체 런북 리뷰, 마지막 업데이트 6개월 이상 알림. 5) 테스트: GameDay에서 런북 따라 대응, 신규 입사자 온보딩에 런북 실습 포함. 6) 담당자 지정: 각 런북 오너 명시, 오너 퇴사 시 인수인계 프로세스. 7) 저장소: Confluence/Notion에 중앙화, 검색 용이성 확보, Git 버전 관리 연동.', ARRAY['Runbook', '런북 자동화', 'ChatOps', '런북 표준화'], '좋은 런북은 경험 적은 담당자도 장애 대응을 가능하게 한다. 테스트되지 않은 런북은 작동하지 않는다고 가정하라.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'capacity-planning', 4, 50, 100, '연간 200% 성장이 예상되는 서비스의 용량 계획 프로세스를 수립하세요. 수요 예측, 부하 테스트, 비용 효율성을 포함하세요.', '1) 수요 예측: 과거 트래픽 데이터 분석, 비즈니스 성장률 반영, 계절성(연말, 이벤트) 고려. 월별 예측 + 분기별 조정. 2) 용량 모델링: 서비스별 리소스 프로파일(요청당 CPU/메모리), 병목 지점(DB, 캐시, 외부 API) 식별. 3) 부하 테스트: 분기별 프로덕션 수준 부하 테스트(k6, Locust), 현재 용량 한계 측정, Breaking Point 확인. 4) N+2 버퍼: 정상 부하 N + 장애 여유 1 + 스파이크 여유 1. 최소 30% 헤드룸 유지. 5) 리드 타임 고려: 대형 DB 확장은 수주 소요, 사전 계획 필수. 6) 비용 최적화: 예측 트래픽의 70%는 Reserved, 30%는 On-demand/Spot. 7) 리뷰 주기: 월간 용량 대시보드 리뷰, 분기별 심층 분석, 연간 전략 수립.', ARRAY['Capacity Planning', '용량 계획', '부하 테스트', 'N+2', '수요 예측'], '용량 계획의 핵심은 리드 타임이다. 용량 부족을 감지한 시점에 확장하면 이미 늦다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'toil-reduction', 4, 50, 100, 'SRE 팀의 Toil이 60%를 초과했습니다. Toil 측정, 자동화 우선순위 결정, 개발팀과 Toil 분담 협의 전략을 제시하세요.', '1) Toil 측정: 2주간 시간 추적, 작업 유형별 분류(배포, 알림 대응, 수동 조작, 데이터 정리). 2) Toil 기준: 수동적, 반복적, 자동화 가능, 전술적, 서비스 성장에 비례, 장기 가치 없음. 3) 우선순위: ROI 계산 = (빈도 × 소요시간 × 팀원수) / 자동화 비용. 높은 ROI부터 자동화. 4) 자동화 타겟: a) 반복 배포 작업 → CI/CD 개선 b) 알림 대응 → 자동 복구 스크립트 c) 데이터 요청 → 셀프 서비스 도구. 5) 개발팀 협의: Toil 데이터로 객관적 논의, 원인이 개발팀 코드에 있으면 함께 해결, PRR에 운영성 항목 추가. 6) 50% 규칙 복원: 단기에 Toil 해결 어려우면 SRE 지원 서비스 축소 협의. 7) 모니터링: 월간 Toil 비율 추적, 목표 설정(분기별 10% 감소).', ARRAY['Toil', '50% 규칙', 'SRE', '자동화', 'ROI'], 'Toil이 50%를 넘으면 자동화할 시간이 없어 악순환에 빠진다. 경영진 지원을 받아 강제로 시간을 확보해야 한다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'canary-analysis', 4, 50, 100, '자동화된 카나리 분석 파이프라인을 설계하세요. 메트릭 선정, 통계적 비교 방법, 자동 롤백 기준, 비즈니스 메트릭 통합을 포함하세요.', '1) 메트릭 선정: 기술 메트릭(에러율, p50/p95/p99 지연, CPU/메모리) + 비즈니스 메트릭(전환율, API 성공률). 2) 실험 설계: 카나리 5%, 베이스라인 5%(같은 조건), 나머지 90% 기존 버전. 최소 1시간 실험. 3) 통계 비교: Mann-Whitney U 테스트로 분포 비교, 신뢰 구간 95%, 효과 크기(Cohen''s d) 고려. 4) 점수 산출: Kayenta 방식으로 각 메트릭 Pass/Fail/Marginal 판정, 가중 평균으로 종합 점수. 5) 자동 롤백: 에러율 2배 이상 증가 즉시 롤백, 종합 점수 70점 미만 롤백, p99 지연 50% 이상 증가 롤백. 6) 점진적 확대: Pass 시 10% → 25% → 50% → 100% 단계적 확대, 각 단계 최소 30분. 7) 알림: 분석 결과 Slack 알림, 실패 시 상세 리포트 자동 생성.', ARRAY['Canary Analysis', 'Kayenta', '통계적 비교', '자동 롤백', '점진적 배포'], '카나리 분석의 핵심은 ''충분히 빨리'' 문제를 감지하면서 ''충분히 정확하게'' 판단하는 것이다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'sre-organization', 4, 50, 100, '100명 규모 엔지니어링 조직에 SRE 팀을 신규 구축하려 합니다. 팀 구조, 채용, 성숙도 단계별 로드맵을 제시하세요.', '1) 팀 규모: 초기 3-5명으로 시작, 개발자 10-15명당 SRE 1명 비율 목표. 2) 팀 구조: 중앙 SRE + 임베디드 하이브리드. 플랫폼 SRE(인프라/도구)와 프로덕트 SRE(서비스별) 분리. 3) 채용: 개발 역량 + 운영 경험, 시스템 사고 능력 중시. 내부 전환(개발→SRE) 적극 활용. 4) 1단계(0-6개월): 관측성 구축, SLI/SLO 정의, 온콜 체계 수립, 런북 작성. 5) 2단계(6-12개월): CI/CD 개선, 토일 자동화, PRR 도입, 에러 버짓 운영 시작. 6) 3단계(12-18개월): 카오스 엔지니어링 도입, 용량 계획 체계화, 서비스 메시 검토. 7) 성공 지표: MTTR 감소, 장애 빈도 감소, 개발자 생산성 향상, 토일 비율 50% 이하.', ARRAY['SRE 조직', 'SRE 채용', '임베디드 SRE', '성숙도 모델'], 'SRE 도입은 기술보다 문화 변화가 어렵다. 경영진 지지, 명확한 성공 지표, 인내심이 필요하다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'multi-region', 4, 50, 100, 'Active-Active 멀티 리전 아키텍처에서 데이터 정합성 이슈가 발생했습니다. 충돌 해결 전략, 리전 간 지연 관리, 장애 시 단일 리전 운영 전략을 설명하세요.', '1) 충돌 유형: 동시 쓰기(같은 레코드 다른 리전에서 수정), 순서 역전(먼저 쓴 게 나중에 도착). 2) 충돌 해결: a) Last Write Wins(타임스탬프 기반) - 단순하지만 데이터 손실 b) Merge(필드별 병합) - 복잡하지만 정확 c) Application-level(비즈니스 규칙) - 가장 정확. 3) 리전 간 지연: 비동기 복제 지연 모니터링, Replication Lag SLO 정의(예: 99%ile 500ms). 4) 충돌 방지: 가능하면 사용자/데이터를 리전에 어피니티(한 리전에서만 쓰기). 5) 단일 리전 운영: 한 리전 장애 시 모든 트래픽 다른 리전으로 이동, 쓰기 지연 증가 수용. 6) 페일백: 원래 리전 복구 후 데이터 동기화 확인, 점진적 트래픽 복귀. 7) 테스트: 분기별 페일오버 훈련, 데이터 정합성 검증 자동화.', ARRAY['Active-Active', '멀티 리전', '충돌 해결', 'Replication Lag', '데이터 정합성'], '멀티 리전 Active-Active의 복잡성은 데이터 계층에서 온다. 가능하면 쓰기를 한 리전으로 제한하는 것이 단순하다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'release-engineering', 4, 50, 100, '일 100회 이상 배포하는 조직의 릴리스 엔지니어링 전략을 수립하세요. 피처 플래그, 롤백 자동화, 배포 안정성 지표를 포함하세요.', '1) 배포 파이프라인: Trunk-based Development, 메인 브랜치 항상 배포 가능, PR 머지 시 자동 배포 트리거. 2) 피처 플래그: LaunchDarkly/Unleash로 코드 배포와 기능 릴리스 분리, 점진적 롤아웃, 문제 시 즉시 기능만 비활성화. 3) 배포 전략: 카나리 → 점진적 확대, 각 단계 자동 메트릭 검증. 4) 롤백 자동화: 배포 후 5분 내 에러율 급증 시 자동 롤백, 롤백 SLA 2분 이내. 5) 안정성 지표: 배포 빈도, 리드 타임(커밋→프로덕션), 변경 실패율, MTTR(DORA 메트릭). 6) 피처 플래그 위생: 30일 이상 활성화된 플래그 정리 알림, 분기별 플래그 정리 스프린트. 7) 변경 동결: 대형 이벤트 전후 배포 금지 기간, 금요일 오후 배포 자제(컬처).', ARRAY['Release Engineering', '피처 플래그', '롤백 자동화', 'DORA 메트릭', '배포 빈도'], '자주 배포할수록 각 배포의 위험이 줄어든다. 하지만 자동화된 안전 장치가 전제되어야 한다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'incident-communication', 4, 50, 100, 'SEV1 장애 발생 시 외부 이해관계자(고객, 파트너, 경영진) 커뮤니케이션 전략을 수립하세요. 타이밍, 메시지 톤, 채널 선택을 포함하세요.', '1) 내부 에스컬레이션: 장애 선언 즉시 경영진(VP, C-level) 알림, 30분 간격 업데이트 또는 상황 변화 시. 2) 고객 커뮤니케이션: 상태 페이지 15분 내 첫 업데이트, ''조사 중/영향 파악 중'' 시작, 추측 배제. 3) 메시지 톤: 사실 기반, 책임 인정, 복구 노력 강조, 기술 용어 최소화. 4) 업데이트 주기: 최소 30분마다 (변화 없어도), ''아직 조사 중''도 업데이트. 5) 복구 후: 사과 + 원인 요약 + 재발 방지 약속 공지. 상세 포스트모텀은 48-72시간 내 공개 검토. 6) 대형 고객: 전담 어카운트 매니저가 직접 연락, 일반 공지 외 개별 브리핑. 7) 미디어/SNS: PR팀과 협조, 공식 입장만 발표, 개인 SNS 발언 자제. 8) 템플릿: 상황별 메시지 템플릿 사전 준비.', ARRAY['Incident Communication', '장애 커뮤니케이션', '상태 페이지', '이해관계자 관리'], '장애 시 침묵은 최악이다. 정보가 부족해도 ''조사 중''이라고 알리는 것이 신뢰를 유지한다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'cascading-failure', 4, 50, 100, '카스케이드 장애(Cascading Failure)가 발생한 상황입니다. 즉각적인 완화 조치, 근본 원인 분석, 재발 방지 아키텍처 개선을 설명하세요.', '1) 즉각 완화: a) 장애 서비스 트래픽 차단(Circuit Breaker Open) b) 비필수 기능 비활성화(Load Shedding) c) 수동 스케일 아웃 d) Rate Limiting 강화. 2) 카스케이드 패턴 파악: A 서비스 느려짐 → B 서비스 타임아웃 누적 → B 스레드 풀 고갈 → B 장애 → C 영향... 3) 근본 원인: 보통 첫 번째 서비스 장애 + 불충분한 타임아웃/재시도/서킷 브레이커 설정. 4) 재발 방지: a) 모든 서비스에 타임아웃 필수(무한 대기 금지) b) 재시도는 지수 백오프 + 최대 횟수 c) 서킷 브레이커 표준 설정 d) Bulkhead로 의존성별 리소스 격리 e) Graceful Degradation(의존성 장애 시 부분 기능 제공). 5) 부하 테스트: 의존성 지연/장애 주입 테스트 정례화.', ARRAY['Cascading Failure', 'Circuit Breaker', 'Bulkhead', 'Load Shedding', '장애 격리'], '카스케이드 장애의 핵심은 ''실패를 예상하고 설계''하는 것이다. 모든 외부 호출은 실패할 수 있다고 가정하라.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'prr', 4, 50, 100, '신규 서비스의 Production Readiness Review(PRR) 체크리스트를 설계하세요. 관측성, 장애 대응, 보안, 확장성 영역을 포함하세요.', '1) 관측성: SLI/SLO 정의됨, 대시보드 구축됨, 알림 설정됨, 분산 트레이싱 적용, 로그 중앙화됨. 2) 장애 대응: 런북 작성됨, 온콜 로테이션 설정됨, 에스컬레이션 경로 정의됨, 롤백 절차 테스트됨. 3) 보안: 인증/인가 구현됨, 시크릿 관리됨(하드코딩 없음), 취약점 스캔 통과, 네트워크 정책 적용됨. 4) 확장성: 부하 테스트 완료, Auto Scaling 설정됨, DB 확장 계획 있음, 용량 예측 완료. 5) 의존성: 외부 의존성 문서화됨, 의존성별 SLA 확인됨, Fallback 전략 있음. 6) 운영: 배포 파이프라인 구축됨, 백업/복구 테스트됨, 비용 태깅됨. 7) 문서: 아키텍처 문서, API 문서, 운영 가이드 존재. 8) 리뷰 프로세스: SRE + 개발팀 합동 리뷰, 블로커 해결 전 프로덕션 불가.', ARRAY['PRR', 'Production Readiness', '체크리스트', '런칭 준비', 'SRE 리뷰'], 'PRR은 프로덕션에서 ''뻔한'' 장애를 예방하는 가장 효과적인 방법이다. 체크리스트는 지속적으로 업데이트되어야 한다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'alert-quality', 4, 50, 100, '알림 피로도(Alert Fatigue)를 해결하기 위한 알림 품질 개선 전략을 수립하세요. 알림 분류, 억제(Inhibition), 중복 제거, SLO 기반 알림 설계를 포함하세요.', '1) 알림 분류: Page(즉시 대응) vs Ticket(업무시간 처리) vs Log(기록만). Page는 액션 가능하고 긴급한 것만. 2) SLO 기반 알림: 개별 메트릭 대신 에러 버짓 소진율 알림. ''CPU 80%''가 아니라 ''SLO 위반 임박''. 3) Burn Rate 알림: 단기 창(1시간) 고소진 → 긴급, 장기 창(6시간) 지속 소진 → 경고. 4) 그룹화: 같은 원인의 다수 알림을 하나로 묶음. ''3개 서버 다운'' 아닌 ''서버 다운 (3대)''. 5) 억제(Inhibition): 상위 장애 발생 시 파생 알림 억제. DB 다운 시 ''DB 연결 실패'' 알림 억제. 6) 중복 제거(Deduplication): 동일 알림 반복 발생 시 첫 번째만, 나머지는 카운트로 표시. 7) 정기 리뷰: 주간 알림 리뷰 미팅, 액션 안 취한 알림 → Ticket 또는 제거, 오탐 원인 분석.', ARRAY['Alert Fatigue', 'SLO 기반 알림', 'Burn Rate', '알림 품질', 'Inhibition'], '좋은 알림은 ''액션 가능하고'', ''긴급하며'', ''정확한'' 것이다. 무시되는 알림은 없느니만 못하다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'data-loss-recovery', 4, 50, 100, '프로덕션 데이터베이스에서 대규모 데이터 삭제 사고가 발생했습니다. 즉각 대응, 복구 전략, 재발 방지 체계를 설명하세요.', '1) 즉각 대응: a) 추가 삭제 방지(쓰기 차단 또는 문제 계정 격리) b) 현재 상태 백업(추가 손상 방지) c) 장애 선언 및 이해관계자 알림. 2) 영향 분석: 삭제된 데이터 범위 파악, 영향받는 사용자/기능 식별. 3) 복구 옵션 평가: a) PITR(Point-in-Time Recovery) - 삭제 직전 시점으로 b) 최신 백업 + 빈로그 리플레이 c) 부분 테이블만 복구 후 merge. 4) 복구 실행: 별도 인스턴스에서 복구 테스트 → 검증 → 프로덕션 적용. 5) 데이터 검증: 복구 후 무결성 검사, 누락 데이터 없는지 확인. 6) 재발 방지: a) 삭제 작업 승인 프로세스 b) 소프트 삭제 정책(즉시 삭제 금지) c) 백업 복구 정기 테스트 d) 프로덕션 직접 쿼리 권한 최소화 e) 위험 쿼리 실행 전 dry-run 필수. 7) 포스트모텀: 사고 원인 분석, 액션 아이템 도출, 전사 공유.', ARRAY['데이터 복구', 'PITR', '데이터 삭제', '백업 복구', '재발 방지'], '데이터 삭제 사고의 최선의 대응은 예방이다. 프로덕션 직접 접근을 최소화하고, 위험 작업은 승인 프로세스를 거치게 하라.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'dependency-management', 4, 50, 100, '핵심 외부 의존성(결제 PG, 지도 API)의 장애 영향을 최소화하는 전략을 수립하세요. Fallback, 이중화, 장애 격리를 포함하세요.', '1) 의존성 인벤토리: 모든 외부 의존성 목록화, SLA/가용성 확인, 비즈니스 크리티컬리티 평가. 2) 이중화: 핵심 의존성은 2개 이상 공급자 계약. PG A 장애 시 PG B로 자동 전환. 3) Fallback 전략: 지도 API 장애 시 캐시된 데이터 사용, SMS 장애 시 이메일로 전환, 추천 API 장애 시 인기 상품 표시. 4) Circuit Breaker: 의존성별 서킷 브레이커, 장애 감지 시 빠른 실패로 리소스 보호. 5) 격리: Bulkhead 패턴으로 의존성별 스레드 풀 분리, 한 의존성 장애가 다른 기능에 영향 주지 않도록. 6) 타임아웃: 외부 호출에 적절한 타임아웃 설정, 무한 대기 방지. 7) 모니터링: 의존성별 가용성/지연 대시보드, SLA 위반 알림. 8) 계약 관리: 공급자와 SLA 명시, 장애 시 보상 조항 확인.', ARRAY['외부 의존성', 'Fallback', 'Circuit Breaker', 'Bulkhead', '이중화'], '외부 의존성은 통제할 수 없다. 언제든 장애가 발생할 수 있다고 가정하고 설계해야 한다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'sre-culture', 4, 50, 100, '조직 전체에 SRE 문화와 신뢰성 마인드셋을 확산시키는 전략을 제시하세요. 교육, 인센티브, 협업 모델을 포함하세요.', '1) 경영진 지지: 신뢰성을 핵심 가치로 선언, SLO를 회사 KPI에 포함, SRE 투자 정당화. 2) 공동 소유: SLO는 개발팀과 SRE 공동 정의/소유, 에러 버짓은 양팀의 공동 책임. 3) 교육: 신입 온보딩에 SRE 원칙 포함, 정기 워크샵(포스트모텀 작성, 장애 대응), 외부 SRE 컨퍼런스 참가 지원. 4) 가시화: 전사 SLO 대시보드, 실시간 에러 버짓 현황, 장애 횟수/MTTR 추이 공유. 5) 인센티브: 신뢰성 개선 성과 인정, 장애 예방 기여 포상, SLO 달성을 평가에 반영. 6) 협업 모델: 임베디드 SRE로 개발팀과 밀착, PRR 통한 협업 기회, 공동 온콜 경험. 7) 지식 공유: 포스트모텀 전사 공유, 장애 학습 세션, ''실패 박람회'' 이벤트. 8) 측정: 분기별 SRE 문화 성숙도 평가, 개발자 설문.', ARRAY['SRE 문화', '신뢰성 마인드셋', '조직 변화', 'SLO 공동 소유'], 'SRE는 팀이 아니라 문화다. 신뢰성은 모든 엔지니어의 책임이라는 인식이 확산되어야 진정한 SRE가 된다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'incident-metrics', 4, 50, 100, '장애 대응 역량을 측정하고 개선하기 위한 메트릭 체계를 설계하세요. MTTR 분해, 장애 트렌드 분석, 벤치마킹 방법을 포함하세요.', '1) 핵심 메트릭: MTTD(탐지), MTTE(에스컬레이션), MTTM(완화), MTTR(복구), 장애 빈도, 장애당 영향 시간(분 × 영향 사용자). 2) MTTR 분해: 전체 MTTR = 탐지 시간 + 대응 시작 시간 + 진단 시간 + 복구 시간. 각 단계별 개선 포인트 식별. 3) 장애 트렌드: 월별/분기별 장애 횟수, SEV별 분포, 원인별 분류(배포, 인프라, 외부), 반복 장애 식별. 4) 서비스별 비교: 서비스별 장애 빈도, 에러 버짓 소진율 비교, 문제 서비스 집중 개선. 5) 벤치마킹: Google SRE 책 기준, DORA 리포트 대비, 업계 평균(가용성 99.9% 등). 6) 리포팅: 월간 신뢰성 리포트 경영진 공유, 분기별 심층 분석, 연간 목표 대비 달성도. 7) 개선 루프: 메트릭 → 근본 원인 분석 → 개선 액션 → 측정 반복.', ARRAY['MTTR', 'MTTD', '장애 메트릭', '신뢰성 측정', '벤치마킹'], '측정하지 않으면 개선할 수 없다. 하지만 메트릭 자체가 목적이 되면 안 된다. 사용자 경험 개선이 궁극적 목표다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'security-incident', 4, 50, 100, '보안 침해 사고(데이터 유출 의심)가 발생했습니다. 초기 대응, 포렌식, 규제 대응, 고객 커뮤니케이션 전략을 수립하세요.', '1) 초기 대응(골든 아워): a) 침해 범위 격리(네트워크 분리, 계정 정지) b) 증거 보존(로그, 메모리 덤프, 디스크 이미지) c) 추가 피해 방지. 2) 팀 구성: 보안팀 + SRE + 법무팀 + PR팀 + 경영진, Incident Commander 지정. 3) 포렌식: 전문 업체 투입 고려, 침입 경로/범위/영향 데이터 파악, 타임라인 재구성. 4) 규제 대응: GDPR 72시간 내 신고 검토, 개인정보보호법 요건 확인, 법무팀 자문. 5) 고객 커뮤니케이션: 영향받는 고객 식별, 사실 확인 후 신속한 개별 통지, 취해야 할 조치(비밀번호 변경 등) 안내. 6) 외부 커뮤니케이션: 공식 성명문 준비, 미디어 대응 창구 일원화, 추측 발언 금지. 7) 복구: 취약점 패치, 침해 경로 차단, 모니터링 강화. 8) 포스트모텀: 보안 강화 액션 아이템, 정기 침투 테스트 도입.', ARRAY['보안 사고', '데이터 유출', '포렌식', 'GDPR', '침해 대응'], '보안 사고 대응의 핵심은 ''증거 보존''과 ''신속한 격리''다. 복구를 서두르다 증거를 훼손하면 원인 분석이 불가능해진다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'sre-investment', 4, 50, 100, 'SRE 투자의 ROI를 경영진에게 설명해야 합니다. 신뢰성 투자의 비용-효과 분석, 정량적 가치 산정, 비즈니스 임팩트 연결 방법을 제시하세요.', '1) 장애 비용 정량화: 분당 매출 손실(총 매출 ÷ 연간 분), 분당 사용자 영향(MAU × 분당 활성률), SLA 위반 페널티, 평판 손상 추정. 2) SRE 효과: MTTR 단축 효과(분 × 분당 비용 절감), 장애 빈도 감소 효과(연간 장애 감소 × 장애당 비용). 3) 예시 계산: 연간 20건 장애 × 평균 30분 × 분당 100만원 = 6억원 손실. SRE 투자로 MTTR 50% 단축 시 3억원 절감. 4) 개발자 생산성: 자동화로 개발팀 운영 작업 감소, 인당 연간 100시간 절감 × 시급 × 개발자 수. 5) 고객 가치: NPS 개선, 이탈률 감소, 엔터프라이즈 계약 SLA 충족. 6) 리스크 감소: 대형 장애 확률 감소, 규제 위반 회피, 데이터 손실 방지. 7) 경쟁 우위: 신뢰성이 마케팅 포인트, 고객 신뢰 구축.', ARRAY['SRE ROI', '신뢰성 투자', '장애 비용', '비즈니스 임팩트', '경영진 설득'], '신뢰성은 비용이 아니라 투자다. 정량적 데이터로 비즈니스 가치를 증명해야 경영진 지지를 얻을 수 있다.', 'docs/08-incident-sre.md');

INSERT INTO questions (category, subcategory, difficulty, level_min, level_max, question_text, correct_answer, keywords, explanation, source_doc) VALUES
('incident', 'major-incident-review', 4, 50, 100, '연간 장애 트렌드 분석 결과 동일 유형의 장애가 반복되고 있습니다. 조직 차원의 반복 장애 근절 전략과 학습 조직 구축 방안을 제시하세요.', '1) 반복 장애 식별: 장애 태깅/분류 체계화(원인별, 서비스별, 영향도별), 분기별 패턴 분석, 상위 5개 반복 유형 도출. 2) 근본 원인 심층 분석: 개별 포스트모텀을 넘어 교차 분석, 공통 기여 요인 식별, 시스템적 약점 파악. 3) 전사 개선 프로그램: 반복 유형별 전담 TF 구성, 분기별 개선 목표 설정, 경영진 스폰서십 확보. 4) 지식 공유: 장애 학습 위키, 월간 장애 리뷰 세션, ''이번 달의 장애'' 공유, 신규 입사자 장애 사례 교육. 5) 예방 투자: 반복 장애 유형에 우선 자동화/모니터링 투자, ROI 기반 우선순위. 6) 문화: 장애를 숨기지 않는 문화, 장애 발견/예방 포상, Blameless 문화 강화. 7) 측정: 반복 장애 비율 KPI 추적, 분기별 목표 대비 달성률, 학습 세션 참여율.', ARRAY['반복 장애', '학습 조직', '장애 패턴', '조직 문화', '포스트모텀'], '같은 장애가 반복되는 것은 개인의 실수가 아니라 조직의 학습 체계 부재다. 시스템적 개선이 필요하다.', 'docs/08-incident-sre.md');
